# -*- coding: utf-8 -*-
"""insider_threat_detection_colab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1x_qT4Rd-OJriTws8KU9mXgqwaIZg7HWG

# Insider Threat Detection using NLP

This notebook implements insider threat detection using:
1. Named Entity Recognition (NER)
2. Sentiment Analysis
3. Combined Classification

## 1. Setup and Dependencies
"""

#!pip install transformers torch pandas numpy scikit-learn seaborn matplotlib tqdm

import torch
from transformers import (
    AutoModelForTokenClassification,
    AutoModelForSequenceClassification,
    AutoTokenizer,
    Trainer,
    TrainingArguments,
    DataCollatorForTokenClassification,
    pipeline
)
from sklearn.metrics import (
    precision_recall_fscore_support,
    accuracy_score,
    classification_report,
    confusion_matrix
)
import pandas as pd
import numpy as np
from typing import List, Optional, Dict
import seaborn as sns
import matplotlib.pyplot as plt
import random
import gc
from tqdm import tqdm

# GPU Setup
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Set up overall seed for reproducibility
seed = 12345
random.seed(seed)
torch.manual_seed(seed)
np.random.seed(seed)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(seed)

"""## 2. Model Implementations"""

# Constants
NER_LABELS = ['O', 'B-ROLE', 'I-ROLE', 'B-FACILITY', 'I-FACILITY', 'B-ACCESS_CODE', 'I-ACCESS_CODE']

def compute_ner_metrics(true_labels, predicted_labels):
    """
    Compute NER-specific metrics

    Args:
        true_labels (List[List[str]]): Ground truth NER labels
        predicted_labels (List[List[str]]): Predicted NER labels

    Returns:
        dict: Dictionary containing precision, recall, f1-score for each entity type
    """
    # Convert labels to flat lists for evaluation
    true_flat = [label for seq in true_labels for label in seq]
    pred_flat = [label for seq in predicted_labels for label in seq]
    
    # Ensure both lists have the same length
    min_len = min(len(true_flat), len(pred_flat))
    true_flat = true_flat[:min_len]
    pred_flat = pred_flat[:min_len]
    
    # Calculate metrics using sklearn's classification_report
    report = classification_report(
        true_flat, 
        pred_flat, 
        output_dict=True,
        zero_division=0
    )
    
    return report

class InsiderThreatDataset:
    def __init__(self, texts: List[str], labels: Optional[List] = None, tokenizer=None):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        return self.texts[idx], self.labels[idx] if self.labels is not None else None

def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1) if len(pred.predictions.shape) > 1 else (pred.predictions > 0.5).astype(int)

    precision, recall, f1, _ = precision_recall_fscore_support(
        labels, preds, average='weighted'
    )
    acc = accuracy_score(labels, preds)

    return {
        'accuracy': acc,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }

class BaseModel:
    def __init__(self, model_name: str, num_labels: int):
        self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = None

    def prepare_data(self, texts: List[str], labels: Optional[List] = None):
        encodings = self.tokenizer(
            texts,
            truncation=True,
            padding=True,
            return_tensors="pt"
        )

        # Move tensors to device
        encodings = {key: value.to(self.device) for key, value in encodings.items()}

        if labels is not None:
            encodings['labels'] = torch.tensor(labels).to(self.device)

        return encodings

    def train(self, train_texts, train_labels, val_texts, val_labels, output_dir,
              num_epochs=3, batch_size=16, learning_rate=2e-5):
        train_encodings = self.prepare_data(train_texts, train_labels)
        val_encodings = self.prepare_data(val_texts, val_labels)

        train_dataset = InsiderThreatDataset(train_encodings, train_labels, self.tokenizer)
        val_dataset = InsiderThreatDataset(val_encodings, val_labels, self.tokenizer)

        training_args = TrainingArguments(
            output_dir=output_dir,
            num_train_epochs=num_epochs,
            per_device_train_batch_size=batch_size,
            per_device_eval_batch_size=batch_size,
            learning_rate=learning_rate,
            evaluation_strategy="epoch",
            save_strategy="epoch",
            load_best_model_at_end=True,
            metric_for_best_model="f1",
        )

        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=val_dataset,
            compute_metrics=compute_metrics,
            data_collator=self.data_collator if hasattr(self, 'data_collator') else None
        )

        trainer.train()
        self.model.save_pretrained(f"{output_dir}/best_model")
        self.tokenizer.save_pretrained(f"{output_dir}/best_model")

class CorporateNERModel(BaseModel):
    def __init__(
            self,
            model_name: str = "microsoft/deberta-v3-base",
            num_labels: int = len(NER_LABELS)
        ):
        super().__init__(model_name, num_labels)
        self.model = AutoModelForTokenClassification.from_pretrained(
            model_name,
            num_labels=num_labels
        ).to(self.device)

        self.data_collator = DataCollatorForTokenClassification(
            tokenizer=self.tokenizer,
            padding=True,
            return_tensors="pt"
        )

    def predict(self, texts: List[str]) -> List[List[str]]:
        self.model.eval()
        encodings = self.prepare_data(texts)

        with torch.no_grad():
            outputs = self.model(**encodings)
            predictions = outputs.logits.argmax(dim=-1)

        return [[NER_LABELS[p.item()] for p in pred_seq] for pred_seq in predictions]

class InsiderThreatLogisticModel(BaseModel):
    def __init__(
            self,
            model_name: str = "cardiffnlp/twitter-roberta-base-sentiment-latest",
        ):
        super().__init__(model_name, num_labels=1)
        self.model = AutoModelForSequenceClassification.from_pretrained(
            model_name,
            num_labels=1,
            problem_type="regression"
        ).to(self.device)

        self.model.classifier = torch.nn.Sequential(
            torch.nn.Linear(self.model.config.hidden_size, 1),
            torch.nn.Sigmoid()
        )

"""## 3. Base Model Evaluation"""

def create_empty_ner_labels(text):
    tokens = text.split()
    return ['O'] * len(tokens)  # 'O' represents "Outside" in NER tagging

def evaluate_ner_base_model(test_data, test_labels, batch_size=8):
    model = CorporateNERModel()
    # Convert to half precision
    model.model = model.model.half()
    model.model.eval()

    all_predictions = []

    # Process in batches
    for i in tqdm(range(0, len(test_data), batch_size)):
        # Clear cache
        torch.cuda.empty_cache()
        gc.collect()

        batch_texts = test_data[i:i + batch_size]
        with torch.no_grad():
            try:
                batch_predictions = model.predict(batch_texts)
                all_predictions.extend(batch_predictions)
            except RuntimeError as e:
                if "out of memory" in str(e):
                    # If OOM occurs, try with even smaller batch
                    torch.cuda.empty_cache()
                    gc.collect()
                    batch_predictions = []
                    for text in batch_texts:
                        pred = model.predict([text])
                        batch_predictions.extend(pred)
                    all_predictions.extend(batch_predictions)
                else:
                    raise e

    return {
        'predictions': all_predictions,
        'metrics': compute_ner_metrics(test_labels, all_predictions)
    }

def evaluate_sentiment_base_model(test_data, test_labels, batch_size=16):
    """Evaluate base sentiment model performance with batch processing"""
    device = 0 if torch.cuda.is_available() else -1
    sentiment_pipe = pipeline(
        "sentiment-analysis",
        model="cardiffnlp/twitter-roberta-base-sentiment-latest",
        device=device,
        model_kwargs={"torch_dtype": torch.float16}  # Use fp16
    )

    all_predictions = []

    # Process in batches
    for i in tqdm(range(0, len(test_data), batch_size)):
        # Clear cache
        torch.cuda.empty_cache()
        gc.collect()

        batch_texts = test_data[i:i + batch_size]
        try:
            batch_predictions = sentiment_pipe(batch_texts)
            all_predictions.extend(batch_predictions)
        except RuntimeError as e:
            if "out of memory" in str(e):
                # If OOM occurs, try with even smaller batch
                torch.cuda.empty_cache()
                gc.collect()
                batch_predictions = []
                for text in batch_texts:
                    pred = sentiment_pipe([text])
                    batch_predictions.extend(pred)
                all_predictions.extend(batch_predictions)
            else:
                raise e

    pred_labels = [1 if pred['label'] == 'POSITIVE' else 0 for pred in all_predictions]

    report = classification_report(test_labels, pred_labels, output_dict=True)
    cm = confusion_matrix(test_labels, pred_labels)

    return {
        'predictions': pred_labels,
        'classification_report': report,
        'confusion_matrix': cm
    }

def plot_confusion_matrix(cm, labels, title='Confusion Matrix'):
    """Plot confusion matrix"""
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=labels, yticklabels=labels)
    plt.title(title)
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.show()

"""## 4. Load and Prepare Data"""

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Set your dataset path here
DATASET_PATH = '/content/drive/MyDrive/cs491_DataAnalyzer/Dataset/synthetic_insider_threat.csv'
MODEL_OUTPUT_DIR = '/content/drive/MyDrive/cs491_DataAnalyzer/Dataset/models'

# Load data
df = pd.read_csv(DATASET_PATH)

# Display sample data
print("Dataset shape:", df.shape)
print("\nSample data:")
display(df.head())

"""## 5. Run Base Model Evaluation"""

# Debug
print("DataFrame columns:", df.columns.tolist())
print("\nDataFrame head:\n", df.head())

# First, prepare sentiment labels and initialize NER labels
df['sentiment'] = df['Is Insider Threat'].map({'Yes': 1, 'No': 0})

# Initialize empty NER labels with proper structure
if 'ner_labels' not in df.columns:
    df['ner_labels'] = df['Tweet'].apply(create_empty_ner_labels)

print("Sample NER labels:", df['ner_labels'].iloc[0])
print("Length of first text:", len(df['Tweet'].iloc[0].split()))
print("Length of first NER labels:", len(df['ner_labels'].iloc[0]))

print("\nEvaluating Base NER Model...")
ner_results = evaluate_ner_base_model(
    df['Tweet'].values.tolist(),
    df['ner_labels'].values.tolist()
)
print("\nNER Model Results:")
print(pd.DataFrame(ner_results['metrics']).T)

print("\nEvaluating Base Sentiment Model...")
sentiment_results = evaluate_sentiment_base_model(
    df['Tweet'].values.tolist(),
    df['sentiment'].values.tolist()
)
print("\nSentiment Model Results:")
print(pd.DataFrame(sentiment_results['classification_report']).T)

plot_confusion_matrix(
    sentiment_results['confusion_matrix'],
    labels=['Negative', 'Positive'],
    title='Base Sentiment Model Confusion Matrix'
)