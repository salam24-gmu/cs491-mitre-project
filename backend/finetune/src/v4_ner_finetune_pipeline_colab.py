# -*- coding: utf-8 -*-
"""v4_ner_finetune_pipeline_colab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1T5HKY0-NQsk8MO0qYQaaO5u0sPvCiaPr
"""

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

!pip install transformers torch pandas tqdm requests numpy evaluate datasets scikit-learn seqeval -q wandb

import os
import json
import torch
import numpy as np
import pandas as pd
from collections import defaultdict, Counter
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple, Any, Optional
import ast
import evaluate
from datasets import Dataset


from sklearn.model_selection import train_test_split
# from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score
)
from sklearn.preprocessing import StandardScaler

from transformers import (
    Trainer,
    TrainingArguments,
    EarlyStoppingCallback,
    AutoTokenizer,
    AutoModelForTokenClassification,
    DataCollatorForTokenClassification,
    pipeline
)

import wandb

wandb.login()

def setup_paths():
    """Set up directory structure and paths for NER pipeline."""
    base_path = "/content/drive/MyDrive/cs491_DataAnalyzer/Dataset"
    paths = {
        'base_path': base_path,
        'training_data_path': os.path.join(base_path, "main_training_dataset"),
        'input_file': os.path.join(base_path, "main_training_dataset", "augmented_synthetic_data.csv"),
        'jsonl_file': os.path.join(base_path, "main_training_dataset", "insider_threat_ner_training.jsonl"),
        'output_file': os.path.join(base_path, "main_training_dataset", "base_ner.csv"),
        'model_dir': os.path.join(base_path, "insider-threat-ner_model")
    }
    os.makedirs(paths['training_data_path'], exist_ok=True)
    os.makedirs(paths['model_dir'], exist_ok=True)
    return paths

# Define entity labels for insider threat detection
INSIDER_THREAT_LABELS = [
    "O",  # Outside any entity
    "B-PERSON", "I-PERSON",
    "B-ORG", "I-ORG",
    "B-LOC", "I-LOC",
    "B-TIME_ANOMALY", "I-TIME_ANOMALY",
    "B-SENSITIVE_INFO", "I-SENSITIVE_INFO",
    "B-TECH_ASSET", "I-TECH_ASSET",
    "B-MEDICAL_CONDITION", "I-MEDICAL_CONDITION",
    "B-SUSPICIOUS_BEHAVIOR", "I-SUSPICIOUS_BEHAVIOR",
    "B-SENTIMENT_INDICATOR", "I-SENTIMENT_INDICATOR"
]

"""1. Core Tokenization Helpers"""

def tokenize_with_offsets(text, tokenizer):
    """
    Tokenize text while maintaining character positions for entity alignment.

    For NER tasks, we need to know exactly which tokens correspond to which
    characters in the original text to correctly assign entity labels.

    Args:
        text (str): Raw text to tokenize
        tokenizer: HuggingFace tokenizer instance

    Returns:
        tuple: (tokens, offsets) where tokens is a list of string tokens and
               offsets is a list of (start, end) character positions
    """
    # Don't add special tokens ([CLS], [SEP]) as they don't correspond to any text
    encoding = tokenizer(text, return_offsets_mapping=True, add_special_tokens=False)
    tokens = tokenizer.convert_ids_to_tokens(encoding['input_ids'])
    offsets = encoding['offset_mapping']
    return tokens, offsets

def compute_max_length(file_path, tokenizer):
    """
    Determine optimal sequence length based on dataset characteristics.

    Finding the right sequence length is important for:
    1. Minimizing padding (improving training efficiency)
    2. Avoiding truncation (preserving information)
    3. Ensuring all sequences fit in GPU memory

    Args:
        file_path (str): Path to JSONL dataset file
        tokenizer: HuggingFace tokenizer instance

    Returns:
        int: Recommended maximum sequence length with 10% buffer
    """
    with open(file_path, 'r') as f:
        data = [json.loads(line) for line in f]

    max_length = 0
    for sample in data:
        # Get token count including special tokens
        tokens = tokenizer.encode(sample["text"])
        max_length = max(max_length, len(tokens))

    # Add 10% buffer to accommodate special tokens and padding
    return int(max_length * 1.1)

def tokenize_and_align_labels(examples, tokenizer, label_list, max_length=128):
    """
    Align entity labels with wordpiece tokens, handling subword tokens appropriately.

    This critical function handles two key NER challenges:
    1. SubToken Alignment: Modern tokenizers split words into subwords, but NER labels
       are assigned at the word level. We need to map labels correctly to subwords.
    2. Special Token Handling: Tokens like [CLS] and [PAD] need special treatment.

    Args:
        examples (dict): Dictionary with keys "tokens" and "ner_tags"
        tokenizer: HuggingFace tokenizer instance
        label_list (list): List of possible entity labels
        max_length (int): Maximum sequence length for padding/truncation

    Returns:
        dict: Tokenized inputs with aligned labels
    """
    # Tokenize pre-split tokens and track word IDs for each resulting subtoken
    tokenized_inputs = tokenizer(
        examples["tokens"],
        is_split_into_words=True,  # Input is already split into words
        truncation=True,
        max_length=max_length,
        padding="max_length",
        return_offsets_mapping=True
    )

    labels = []
    for i, label in enumerate(examples["ner_tags"]):
        # Get word_ids mapping each subtoken back to its original word
        # Example: "John" → ["John"] but "unbelievable" → ["un", "##believable"]
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        previous_word_idx = None
        label_ids = []

        for word_idx in word_ids:
            if word_idx is None:
                # Special tokens ([CLS], [SEP], [PAD]) get ignored in loss calculation
                label_ids.append(-100)
            elif word_idx != previous_word_idx:
                # First subtoken of a word gets the actual label
                label_idx = label_list.index(label[word_idx]) if word_idx < len(label) else 0
                label_ids.append(label_idx)
            else:
                # Continuation subtokens are masked (-100) to avoid counting them in loss
                # This prevents artificially inflating model performance
                label_ids.append(-100)
            previous_word_idx = word_idx

        labels.append(label_ids)

    tokenized_inputs["labels"] = labels
    return tokenized_inputs

"""2. Dataset Loading & Preparation"""

def load_jsonl_dataset(file_path, tokenizer):
    """
    Map character-based entity annotations to token-based labels that the model can use for training.

    Args:
        file_path (str): Path to JSONL dataset with text and entity annotations
        tokenizer: HuggingFace tokenizer instance

    Returns:
        Dataset: HuggingFace Dataset with tokenized inputs and aligned labels
    """
    print(f"Loading dataset from {file_path}")

    data = []
    with open(file_path, 'r') as f:
        for line in f:
            data.append(json.loads(line))
    print(f"Loaded {len(data)} documents")

    processed_data = []
    for sample in data:
        # Tokenize text and get character offsets for each token
        tokens, offsets = tokenize_with_offsets(sample["text"], tokenizer)

        # Initialize all token labels as 'O' (Outside any entity)
        ner_tags = ["O"] * len(tokens)

        # Map entities to tokens using character offsets
        # Sort entities by start position to handle overlapping entities consistently
        for entity in sorted(sample.get("entities", []), key=lambda x: x["start"]):
            ent_start, ent_end, ent_label = entity["start"], entity["end"], entity["label"]

            # Find all tokens that overlap with this entity
            for idx, (token_start, token_end) in enumerate(offsets):
                if token_start >= ent_start and token_end <= ent_end:
                    # Use BIO scheme: B- prefix for first token, I- for continuation
                    prefix = "B-" if token_start == ent_start else "I-"
                    ner_tags[idx] = f"{prefix}{ent_label}"

        processed_data.append({
            "tokens": tokens,
            "ner_tags": ner_tags,
            "text": sample["text"],
            "entities": sample.get("entities", [])
        })

    return Dataset.from_list(processed_data)

def prepare_train_val_test(dataset, test_size=0.1, val_size=0.15):
    """Split dataset into training, validation, and test sets."""
    # First split: create test set
    train_val = dataset.train_test_split(test_size=test_size, seed=42)
    train_set = train_val["train"]
    test_set = train_val["test"]

    # Second split: create validation set from remaining data
    relative_val_size = val_size / (1 - test_size)
    train_val_split = train_set.train_test_split(test_size=relative_val_size, seed=42)
    train_set = train_val_split["train"]
    val_set = train_val_split["test"]

    print(f"Training set: {len(train_set)} samples")
    print(f"Validation set: {len(val_set)} samples")
    print(f"Test set: {len(test_set)} samples")

    return train_set, val_set, test_set

"""3. NERTrainer Class"""

class NERTrainer:
    """
    This encapsulates the entire NER model lifecycle:
    1. Dataset preparation with token-label alignment
    2. Model configuration and training
    3. Evaluation with NER-specific metrics
    """

    def __init__(self, model_name="dslim/bert-base-NER"):
        self.model_name = model_name
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)

    def prepare_datasets(self, train_set, val_set, max_length):
        """
        Args:
            train_set (Dataset): Raw training dataset
            val_set (Dataset): Raw validation dataset
            max_length (int): Maximum sequence length

        Returns:
            tuple: (tokenized_train, tokenized_val) ready for model training
        """
        # Process training data with aligned labels
        tokenized_train = train_set.map(
            lambda examples: tokenize_and_align_labels(
                examples, self.tokenizer, INSIDER_THREAT_LABELS, max_length
            ),
            batched=True,
            remove_columns=train_set.column_names
        )

        # Process validation data with aligned labels
        tokenized_val = val_set.map(
            lambda examples: tokenize_and_align_labels(
                examples, self.tokenizer, INSIDER_THREAT_LABELS, max_length
            ),
            batched=True,
            remove_columns=val_set.column_names
        )

        return tokenized_train, tokenized_val

    def compute_metrics(self, p):
        """
        Calculate NER-specific evaluation metrics using seqeval.

        Seqeval implements entity-level metrics that properly account for
        entity boundaries, unlike token-level metrics that can be misleading
        for NER tasks.

        Args:
            p: Prediction object containing model predictions and labels

        Returns:
            dict: Dictionary of computed metrics (precision, recall, f1, accuracy)
        """
        # Load the seqeval metric for entity-level evaluation
        metric = evaluate.load("seqeval")
        predictions, labels = p

        # Convert logits to predicted class indices
        # Shape: (batch_size, seq_length, num_classes) → (batch_size, seq_length)
        predictions = np.argmax(predictions, axis=2)

        # Remove ignored indices (-100) from evaluation
        # This handles special tokens and continuation subtokens
        true_predictions = [
            [INSIDER_THREAT_LABELS[p] for (p, l) in zip(prediction, label) if l != -100]
            for prediction, label in zip(predictions, labels)
        ]
        true_labels = [
            [INSIDER_THREAT_LABELS[l] for (p, l) in zip(prediction, label) if l != -100]
            for prediction, label in zip(predictions, labels)
        ]

        # Compute entity-level metrics
        results = metric.compute(predictions=true_predictions, references=true_labels)

        return {
            "precision": results["overall_precision"],
            "recall": results["overall_recall"],
            "f1": results["overall_f1"],
            "accuracy": results["overall_accuracy"],
        }

    def train_with_wandb(self, train_set, val_set, max_length, output_dir):
        """
        Train NER model with WandB-optimized hyperparameters.

        This function handles the complete training process including:
        1. Dataset preparation
        2. Model initialization with custom label set
        3. Training configuration and execution
        4. Evaluation and model saving

        Args:
            train_set (Dataset): Training dataset
            val_set (Dataset): Validation dataset
            max_length (int): Maximum sequence length
            output_dir (str): Directory to save model and results

        Returns:
            tuple: (eval_results, model_save_path) - Evaluation metrics and path to saved model
        """
        # Get hyperparameters from wandb
        config = wandb.config

        # Prepare datasets with token-label alignment
        tokenized_train, tokenized_val = self.prepare_datasets(train_set, val_set, max_length)

        # Initialize model with our custom entity label set
        model = AutoModelForTokenClassification.from_pretrained(
            self.model_name,
            num_labels=len(INSIDER_THREAT_LABELS),
            id2label={i: label for i, label in enumerate(INSIDER_THREAT_LABELS)},
            label2id={label: i for i, label in enumerate(INSIDER_THREAT_LABELS)},
            ignore_mismatched_sizes=True  # Handle architecture differences
        )

        # Configure training parameters
        training_args = TrainingArguments(
            output_dir=os.path.join(output_dir, f"run-{wandb.run.id}"),
            evaluation_strategy="epoch",  # Evaluate after each epoch
            save_strategy="epoch",        # Save checkpoint after each epoch
            learning_rate=config.learning_rate,
            per_device_train_batch_size=config.batch_size,
            per_device_eval_batch_size=config.batch_size,
            num_train_epochs=config.epochs,
            weight_decay=config.weight_decay,  # L2 regularization
            load_best_model_at_end=True,       # Restore best checkpoint after training
            metric_for_best_model="f1",        # Optimize for F1 score
            greater_is_better=True,            # Higher F1 is better
            report_to="wandb",                 # Log metrics to wandb
            fp16=torch.cuda.is_available(),    # Use mixed precision if GPU available
            dataloader_num_workers=2,          # Parallel data loading
            warmup_ratio=0.1                   # Gradually ramp up learning rate
        )

        # Prepare special components for NER training
        # DataCollator handles batching and padding tokens consistently
        data_collator = DataCollatorForTokenClassification(self.tokenizer)

        # Early stopping prevents overfitting by halting training when metrics plateau
        callbacks = [EarlyStoppingCallback(
            early_stopping_patience=config.early_stopping_patience
        )]

        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=tokenized_train,
            eval_dataset=tokenized_val,
            tokenizer=self.tokenizer,
            data_collator=data_collator,
            compute_metrics=self.compute_metrics,
            callbacks=callbacks
        )

        print("\nStarting model training...")
        train_results = trainer.train()

        wandb.log({
            "train_runtime": train_results.metrics["train_runtime"],
            "train_samples_per_second": train_results.metrics["train_samples_per_second"],
            "train_loss": train_results.metrics["train_loss"]
        })

        # Evaluate final model on validation set
        print("\nEvaluating model...")
        eval_results = trainer.evaluate()
        print(f"Evaluation results: {eval_results}")

        # Save model, tokenizer, and configuration
        model_save_path = os.path.join(output_dir, f"model-{wandb.run.id}")
        trainer.save_model(model_save_path)
        self.tokenizer.save_pretrained(model_save_path)

        return eval_results, model_save_path

"""4. WandB Sweep Configuration"""

def setup_wandb_sweep():
    """
    Configure Bayesian hyperparameter optimization sweep for NER.

    Bayesian optimization adaptively explores the hyperparameter space,
    focusing on promising regions based on previous trial results.
    This is more efficient than random or grid search for finding optimal settings.

    Returns:
        str: Sweep ID for tracking and agent control
    """
    sweep_config = {
        'method': 'bayes',  # Bayesian optimization learns from past trials
        'metric': {
            'name': 'eval_f1',  # Optimize for F1 score
            'goal': 'maximize'  # Higher is better
        },
        'parameters': {
            'learning_rate': {
                'distribution': 'uniform',  # Sample uniformly in range
                'min': 1e-5,
                'max': 5e-5
            },
            'batch_size': {
                'values': [8, 16, 32]
            },
            'epochs': {
                'values': [3, 5, 7, 10]  # Discrete options for training duration
            },
            'weight_decay': {
                'values': [0.01, 0.001, 0.0001]  # L2 regularization strengths
            },
            'early_stopping_patience': {
                'values': [2, 3]  # How many epochs to wait for improvement
            }
        }
    }

    # Create sweep and get identifier
    sweep_id = wandb.sweep(sweep_config, project="insider-threat-ner")
    return sweep_id

"""5. Main Training Function"""

def train_ner_model():
    """
    Execute end-to-end NER training process with WandB integration.

    This function orchestrates the complete training lifecycle:
    1. Initialize wandb run for tracking
    2. Set up paths and load data
    3. Determine appropriate sequence length
    4. Train model with hyperparameter optimization
    5. Save model and metadata

    Returns:
        dict: Evaluation results from validation set
    """
    run = wandb.init()
    paths = setup_paths()
    ner_trainer = NERTrainer()

    # Calculate appropriate maximum sequence length from dataset
    # This prevents unnecessary padding or information loss from truncation
    max_length = compute_max_length(paths['jsonl_file'], ner_trainer.tokenizer)
    print(f"Using maximum sequence length: {max_length}")

    dataset = load_jsonl_dataset(paths['jsonl_file'], ner_trainer.tokenizer)
    train_set, val_set, _ = prepare_train_val_test(dataset)

    # Train model with current hyperparameter configuration
    eval_results, model_path = ner_trainer.train_with_wandb(
        train_set,
        val_set,
        max_length,
        paths['model_dir']
    )

    # Save metadata for model comparison and selection
    with open(os.path.join(paths['model_dir'], f"model_info_{wandb.run.id}.json"), 'w') as f:
        json.dump({
            'model_path': model_path,
            'wandb_run_id': wandb.run.id,
            'eval_results': eval_results
        }, f)

    wandb.finish()
    return eval_results

def run_sweep_agent(sweep_id, count=10):
    """
    Run WandB sweep agent for hyperparameter optimization.

    The agent executes multiple training runs, each with different
    hyperparameter configurations determined by the Bayesian optimizer.

    Args:
        sweep_id (str): WandB sweep identifier
        count (int): Number of trials to run
    """
    wandb.agent(sweep_id, train_ner_model, count=count)

"""6. Evaluation Functions"""

def evaluate_entities(entities, tweet_text, tweet_timestamp=None):
    """
    Organizes detected entities by type and provides confidence scores.
    Additionally, flags high-risk entity combinations and checks the tweet
    timestamp to flag off-hours activity (outside 9AM-5PM).

    Args:
        entities (list): List of entity dictionaries from the NER pipeline.
        tweet_text (str): The tweet text that was analyzed.
        tweet_timestamp (str, optional): Timestamp string in the format '%Y-%m-%d %H:%M:%S'.

    Returns:
        str: Formatted string with entity analysis results and flagged high-risk indicators.
    """
    if not entities:
        return f"No entities found in tweet: {tweet_text}"
    # Group entities by type.
    entity_types = {}
    for entity in entities:
        entity_type = entity["entity_group"]
        if entity_type not in entity_types:
            entity_types[entity_type] = []
        entity_types[entity_type].append(entity)

    flagged_indicators = []
    if "SUSPICIOUS_BEHAVIOR" in entity_types and "SENSITIVE_INFO" in entity_types:
        flagged_indicators.append("SUSPICIOUS_BEHAVIOR + SENSITIVE_INFO")
    if "TIME_ANOMALY" in entity_types and "TECH_ASSET" in entity_types:
        flagged_indicators.append("TIME_ANOMALY + TECH_ASSET")
    if "SUSPICIOUS_BEHAVIOR" in entity_types and "TIME_ANOMALY" in entity_types:
        flagged_indicators.append("SUSPICIOUS_BEHAVIOR + TIME_ANOMALY")

    # Check off-hours: parse timestamp and flag if outside 9AM-5PM.
    if tweet_timestamp is not None:
        try:
            dt = datetime.strptime(tweet_timestamp, '%Y-%m-%d %H:%M:%S')
            if dt.hour < 9 or dt.hour >= 17:
                flagged_indicators.append("Off-Hours Activity Detected")
        except Exception as e:
            flagged_indicators.append("Timestamp Parsing Error")
    result = f"Tweet: {tweet_text}\n\nEntities detected:\n"
    for entity_type, ents in entity_types.items():
        result += f"\n{entity_type}:\n"
        for entity in ents:
            result += f"  - {entity['word']} (Score: {entity['score']:.4f})\n"
    if flagged_indicators:
        result += "\nFlagged High-Risk Indicators:\n"
        for indicator in flagged_indicators:
            result += f"  - {indicator}\n"
    else:
        result += "\nNo high-risk indicators flagged.\n"
        result += "Recommendation: Verify detection thresholds and combination rules for high-risk entities.\n"
    return result

def find_best_model():
    """
    Scans through saved model metadata files to identify the best-performing NER model
    (based on the highest F1 score on the validation set).

    Returns:
        str: Path to the best model directory, or None if no models found.
    """
    paths = setup_paths()
    model_info_files = [f for f in os.listdir(paths['model_dir']) if f.startswith('model_info_')]
    best_f1 = 0
    best_model_path = None
    for info_file in model_info_files:
        with open(os.path.join(paths['model_dir'], info_file), 'r') as f:
            info = json.load(f)
        f1_score = info['eval_results'].get('eval_f1', 0)
        if f1_score > best_f1:
            best_f1 = f1_score
            best_model_path = info['model_path']
    print(f"Best model found with F1 score: {best_f1}")
    return best_model_path

def evaluate_best_model(test_text=None, batch_size=8):
    """
    Evaluate the best NER model on test data or a provided tweet.
    If no test_text is provided, the entire test set is processed.

    Args:
        test_text (str, optional): Custom tweet text to evaluate.
        batch_size (int): Size of batches for processing multiple examples.

    Returns:
        str or list: Formatted results of entity detection.
    """
    paths = setup_paths()
    best_model_path = find_best_model()
    if best_model_path is None:
        print("No model found. Please run the training/sweep first.")
        return
    print(f"Loading best model: {best_model_path}")

    tokenizer = AutoTokenizer.from_pretrained(best_model_path)
    model = AutoModelForTokenClassification.from_pretrained(best_model_path)

    ner_pipeline = pipeline("ner", model=model, tokenizer=tokenizer,
                           aggregation_strategy="simple", device=0)
    if test_text:
        entities = ner_pipeline(test_text)
        return evaluate_entities(entities, test_text)
    else:
        dataset = load_jsonl_dataset(paths['jsonl_file'], tokenizer)
        _, _, test_set = prepare_train_val_test(dataset)

        texts = [example["text"] for example in test_set]
        timestamps = [example.get("timestamp") for example in test_set]


        results = []
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i+batch_size]
            batch_timestamps = timestamps[i:i+batch_size]

            # Process batch
            batch_entities = ner_pipeline(batch_texts)

            # Handle results for each item in batch
            for j, entities in enumerate(batch_entities):
                results.append(evaluate_entities(
                    entities,
                    batch_texts[j],
                    tweet_timestamp=batch_timestamps[j]
                ))

        return results

""" 7. Run Training (Optional - Intensive Process)"""

# Run this cell if you want to train models using hyperparameter sweeps
# WARNING: This is a computationally intensive process and may take a while

# Create and run the sweep
sweep_id = setup_wandb_sweep()
run_sweep_agent(sweep_id, count=10)  # Adjust count as needed

"""8. Evaluate Best Model"""

# Evaluate the best model from previous training runs
print("\n===== Evaluating Best Model =====")
results = evaluate_best_model()
for result in results:
    print("\n" + result)

# Alternatively, test on a specific example
# custom_text = "I'm going to download the customer database tonight after everyone has gone home."
# print(evaluate_best_model(custom_text))

"""# Risk Profiling

1. The code first extracts entity information from tweet data (which might come from different NER sources) and standardizes it into a common format. It also groups similar entities to reduce redundancy.

2. Each entity is assigned a risk weight based on its type (e.g., suspicious behavior, sensitive info). Then, tweet-level risk scores are computed by combining individual entity risks—adding extra multipliers when high-risk combinations are detected.

3. Individual tweet risks are aggregated to build a user-level profile. This profile includes total risk, average risk per tweet, standard deviations, entity counts, and distributions.

4. The aggregated profiles are converted into fixed-length feature vectors suitable for machine learning. Normalization (min–max or z-score) and outlier detection are applied to standardize the input space.
"""

# ============== 1. Entity Extraction and Standardization ==============

def extract_fallback_entities(text):
    """
    Basic fallback entity extraction for cases where no entities were detected.
    Uses simple pattern matching to identify potential entities.
    """
    fallback_entities = []

    # Simple keyword-based detection for common entity types
    tech_patterns = ["server", "database", "system", "network", "access", "login", "password",
                    "file", "document", "data", "computer", "laptop"]

    sensitive_patterns = ["confidential", "private", "secret", "personal", "sensitive",
                        "restricted", "classified", "patient", "record", "financial"]

    suspicious_patterns = ["delete", "hide", "bypass", "override", "unauthorized", "after hours",
                         "no one will know", "don't tell", "between us", "off the record"]

    time_patterns = ["late", "night", "weekend", "early", "after hours", "before opening",
                   "midnight", "sunday", "saturday", "holiday"]

    # Check for tech assets
    for pattern in tech_patterns:
        if pattern in text.lower():
            fallback_entities.append({
                "entity_type": "TECH_ASSET",
                "entity_text": pattern,
                "confidence": 0.75
            })

    # Check for sensitive info
    for pattern in sensitive_patterns:
        if pattern in text.lower():
            fallback_entities.append({
                "entity_type": "SENSITIVE_INFO",
                "entity_text": pattern,
                "confidence": 0.8
            })

    # Check for suspicious behavior
    for pattern in suspicious_patterns:
        if pattern in text.lower():
            fallback_entities.append({
                "entity_type": "SUSPICIOUS_BEHAVIOR",
                "entity_text": pattern,
                "confidence": 0.85
            })

    # Check for time anomalies
    for pattern in time_patterns:
        if pattern in text.lower():
            fallback_entities.append({
                "entity_type": "TIME_ANOMALY",
                "entity_text": pattern,
                "confidence": 0.8
            })

    return fallback_entities


def extract_and_standardize_entities(data, confidence_threshold=0.65):
    """
    Unifies diverse entity detection formats into a standardized representation,
    critical for consistent downstream analysis. This handles both tuple-based
    representations from rule engines and dictionary-based NER model outputs,
    establishing a common ground for risk assessment regardless of source.
    """
    processed_tweets = []

    for i, tweet_data in enumerate(data):
        tweet_id = tweet_data.get("tweet_id", f"tweet_{i}")
        user_id = tweet_data.get("user_id", "unknown_user")
        text = tweet_data.get("tweet", "")
        timestamp = tweet_data.get("timestamp", None)
        entities_data = tweet_data.get("entities", [])
        std_entities = []

        # Handle dictionary-based NER model outputs (common in spaCy, HuggingFace, etc.)
        if isinstance(entities_data, list) and entities_data and isinstance(entities_data[0], tuple):
            for entity_text, entity_type in entities_data:
                confidence = 0.9  # Assume high confidence for rule-based entities
                std_word = entity_text.lower().strip()
                std_entities.append({
                    "entity_type": entity_type,
                    "entity_text": std_word,
                    "confidence": confidence
                })
        # Handle tuple-based outputs from rule engines (common in older systems)
        else:
            for entity in entities_data:
                if isinstance(entity, dict):
                    entity_type = entity.get("entity_group", entity.get("label", ""))
                    confidence = entity.get("score", 0.0)
                    word = entity.get("word", "")

                    if confidence < confidence_threshold:
                        continue

                    std_word = word.lower().strip()
                    std_entities.append({
                        "entity_type": entity_type,
                        "entity_text": std_word,
                        "confidence": confidence
                    })

        # Add validation for empty entity lists
        if not std_entities:
            # Apply basic entity extraction for empty entities
            std_entities = extract_fallback_entities(text)

        metadata = {
            "classification": tweet_data.get("classification", None),
            "sentiment": tweet_data.get("sentiment", None),
            "off_hours": tweet_data.get("off_hours", None)
        }

        grouped_entities = group_similar_entities(std_entities)

        processed_tweet = {
            "tweet_id": tweet_id,
            "user_id": user_id,
            "text": text,
            "timestamp": timestamp,
            "entities": grouped_entities,
            "metadata": metadata
        }

        processed_tweets.append(processed_tweet)

    return processed_tweets



def group_similar_entities(entities):
    """
    Consolidates semantically related entity mentions to reduce redundancy
    and noise in the feature space. Uses substring relationships to identify
    overlapping references and prioritizes longer, higher-confidence mentions
    to create a better representation for signal for downstream risk analysis.
    """
    sorted_entities = sorted(entities, key=lambda x: x["confidence"], reverse=True)
    entity_groups = {}

    for entity in sorted_entities:
        entity_type = entity["entity_type"]
        entity_text = entity["entity_text"]

        is_substring = False
        for existing_text in list(entity_groups.get(entity_type, {}).keys()):
            if entity_text in existing_text or existing_text in entity_text:
                if len(entity_text) > len(existing_text):
                    new_confidence = max(entity["confidence"],
                                        entity_groups[entity_type][existing_text]["confidence"])

                    entity_groups.setdefault(entity_type, {})[entity_text] = {
                        "entity_type": entity_type,
                        "entity_text": entity_text,
                        "confidence": new_confidence
                    }
                    del entity_groups[entity_type][existing_text]
                else:
                    entity_groups[entity_type][existing_text]["confidence"] = max(
                        entity["confidence"],
                        entity_groups[entity_type][existing_text]["confidence"]
                    )
                is_substring = True
                break

        if not is_substring:
            entity_groups.setdefault(entity_type, {})[entity_text] = entity

    grouped_entities = []
    for type_dict in entity_groups.values():
        grouped_entities.extend(list(type_dict.values()))

    return grouped_entities

# ============== 2. Risk-Weighted Entity Quantification ==============

def refined_entity_risk_weights():
    """
    Provides more granular and context-aware entity risk weights based on
    real-world security implications.
    """
    return {
        # Highest risk entities
        "SUSPICIOUS_BEHAVIOR": 0.95,
        "SENSITIVE_INFO": 0.90,

        # High risk entities
        "TIME_ANOMALY": 0.75,
        "TECH_ASSET": 0.70,

        # Medium risk entities
        "MEDICAL_CONDITION": 0.60,
        "SENTIMENT_INDICATOR": 0.55,

        # Base entities
        "PERSON": 0.35,  # Increased as mentioning specific people can be sensitive
        "ORG": 0.40,     # Increased as mentioning organizations can indicate targeting
        "LOC": 0.20
    }

def assign_risk_weights(standardized_tweets, entity_risk_weights=None):
    """
    Applies domain-specific risk weights to entities based on their type,
    reflecting varying levels of security concern. The weighting system
    prioritizes suspicious behaviors and sensitive information over benign
    entities, allowing for calibrated risk quantification aligned with
    security psychology models.
    """
    if entity_risk_weights is None:
        entity_risk_weights = refined_entity_risk_weights()

    risk_weighted_tweets = []

    for tweet in standardized_tweets:
        weighted_tweet = tweet.copy()
        weighted_entities = []
        has_precalculated_risk = tweet.get("metadata", {}).get("risk_score") is not None

        for entity in tweet["entities"]:
            weighted_entity = entity.copy()
            entity_type = entity["entity_type"]
            risk_weight = entity_risk_weights.get(entity_type, 0.1)
            entity_risk = entity["confidence"] * risk_weight
            weighted_entity["risk_score"] = entity_risk
            weighted_entity["risk_weight"] = risk_weight
            weighted_entities.append(weighted_entity)

        weighted_tweet["entities"] = weighted_entities

        if has_precalculated_risk:
            weighted_tweet["precalculated_risk"] = weighted_tweet["metadata"]["risk_score"]

        risk_weighted_tweets.append(weighted_tweet)

    return risk_weighted_tweets

# ============== 3. Tweet-Level Risk Assessment ==============
def context_aware_risk_assessment(text):
    """
    Analyzes text context for risk signals beyond entity detection.

    Args:
        text (str): Text to analyze

    Returns:
        tuple: (risk_score, context_signals) where risk_score is a float
               and context_signals is a list of detected signals
    """
    context_signals = []
    risk_score = 0.0

    # Check for patterns indicating security context
    security_phrases = ["security", "access", "clearance", "permission", "classified",
                       "authorization", "authorized", "confidential", "restricted"]

    # Check for patterns indicating secrecy or covert behavior
    secrecy_phrases = ["secret", "private", "hide", "delete", "erase", "remove",
                      "nobody", "no one", "between us", "don't tell", "keep this quiet"]

    # Check for financial motivation
    financial_phrases = ["money", "payment", "cash", "pay", "debt", "financial",
                        "opportunity", "offer", "compensation"]

    # Count occurrences of each category
    security_count = sum(1 for phrase in security_phrases if phrase in text.lower())
    secrecy_count = sum(1 for phrase in secrecy_phrases if phrase in text.lower())
    financial_count = sum(1 for phrase in financial_phrases if phrase in text.lower())

    # Add signals based on occurrence
    if security_count > 0:
        context_signals.append("SECURITY_CONTEXT")
        risk_score += 0.1 * min(security_count, 3)  # Cap at 0.3

    if secrecy_count > 0:
        context_signals.append("SECRECY_INDICATORS")
        risk_score += 0.15 * min(secrecy_count, 3)  # Cap at 0.45

    if financial_count > 0:
        context_signals.append("FINANCIAL_MOTIVATION")
        risk_score += 0.1 * min(financial_count, 3)  # Cap at 0.3

    # Check for combination of security context and secrecy
    if security_count > 0 and secrecy_count > 0:
        context_signals.append("SECURITY_WITH_SECRECY")
        risk_score += 0.2  # Additional risk for this combination

    return risk_score, context_signals

def semantic_risk_analysis(text):
    """
    Performs semantic analysis of text to identify risk indicators.

    Args:
        text (str): Text to analyze

    Returns:
        tuple: (risk_score, indicators) with semantic risk assessment
    """
    indicators = []
    risk_score = 0.0

    # Check for negative sentiment words
    # TODO: implement Word2Vec DIC enlargment
    negative_words = ["angry", "frustrated", "disappointed", "upset", "hate",
                     "stupid", "useless", "terrible", "awful", "annoyed"]

    # Check for threatening language
    threatening_words = ["threat", "revenge", "get back", "pay for this",
                        "they'll see", "they will regret", "make them sorry"]

    # Check for personal pronouns with negative context (indication of personal grievance)
    personal_negative = ["I hate", "I quit", "I'm leaving", "they fired",
                        "my boss", "they don't appreciate", "I deserve"]

    # Count occurrences
    negative_count = sum(1 for word in negative_words if word in text.lower())
    threatening_count = sum(1 for word in threatening_words if word in text.lower())
    personal_count = sum(1 for phrase in personal_negative if phrase.lower() in text.lower())

    # Add indicators and score based on occurrences
    if negative_count > 0:
        indicators.append("NEGATIVE_SENTIMENT")
        risk_score += 0.05 * min(negative_count, 4)  # Cap at 0.2

    if threatening_count > 0:
        indicators.append("THREATENING_LANGUAGE")
        risk_score += 0.15 * min(threatening_count, 3)  # Cap at 0.45

    if personal_count > 0:
        indicators.append("PERSONAL_GRIEVANCE")
        risk_score += 0.1 * min(personal_count, 3)  # Cap at 0.3

    # Check for combinations
    if negative_count > 0 and personal_count > 0:
        indicators.append("PERSONAL_NEGATIVITY")
        risk_score += 0.1  # Additional risk for this combination

    if threatening_count > 0 and personal_count > 0:
        indicators.append("PERSONAL_THREAT")
        risk_score += 0.25  # Significant risk increase for this combination

    return risk_score, indicators

def improve_risk_combo_detection():
    """
    Enhanced risk combination detection with expanded combinations and dynamic thresholds.
    """
    return {
        # Original combinations with adjusted multipliers
        ("SUSPICIOUS_BEHAVIOR", "SENSITIVE_INFO"): 1.8,
        ("TIME_ANOMALY", "TECH_ASSET"): 1.5,
        ("SUSPICIOUS_BEHAVIOR", "SUSPICIOUS_BEHAVIOR"): 1.4,

        # New combinations that signal potential data exfiltration
        ("SENSITIVE_INFO", "TECH_ASSET"): 1.6,
        ("SUSPICIOUS_BEHAVIOR", "ORG"): 1.3,
        ("SENSITIVE_INFO", "TIME_ANOMALY"): 1.7,

        # Additional context-aware combinations
        ("PERSON", "SENSITIVE_INFO"): 1.3,
        ("SENTIMENT_INDICATOR", "SUSPICIOUS_BEHAVIOR"): 1.2,
        ("TIME_ANOMALY", "SUSPICIOUS_BEHAVIOR"): 1.4
    }

def enhanced_time_analysis(tweet_timestamp, tweet_text=None, time_anomaly_present=False):
    from datetime import datetime
    """
    Improved time analysis that considers both timestamp and content.

    Args:
        tweet_timestamp: The timestamp string
        tweet_text: The text of the tweet for content analysis
        time_anomaly_present: Boolean indicating if a TIME_ANOMALY entity was detected

    Returns:
        Boolean indicating if this is off-hours activity
    """
    # First check if there's explicit time anomaly detection
    if time_anomaly_present:
        return True

    # Check the timestamp
    if tweet_timestamp is None:
        return False

    try:
        dt = datetime.strptime(tweet_timestamp, '%Y-%m-%d %H:%M:%S')

        # Weekend check
        if dt.weekday() >= 5:  # 5=Saturday, 6=Sunday
            return True

        # Expanded definition (before 8AM or after 5PM)
        # TODO: Extract directly from DT
        if dt.hour < 8 or dt.hour >= 17:
            return True

        return False
    except Exception:
        # Try alternative formats
        try:
            # Try alternative formats
            alternative_formats = ['%m/%d/%Y %H:%M:%S', '%d-%m-%Y %H:%M:%S', '%Y/%m/%d %H:%M:%S']
            for fmt in alternative_formats:
                try:
                    dt = datetime.strptime(tweet_timestamp, fmt)
                    # Same checks as above
                    return dt.weekday() >= 5 or dt.hour < 8 or dt.hour >= 17
                except:
                    continue
            return False
        except:
            return False

def assess_tweet_risk(risk_weighted_tweets, risk_multipliers=None, use_precalculated=False):
    """
      Calculates multiple risk dimensions for a single tweet.
        1. Base Risk: Sum of individual entity risk_score values.
        2. Contextual Risk: Calculated by `context_aware_risk_assessment` by looking for keywords related to security, secrecy, or financial motivation in the tweet text.
        3. Semantic Risk: Calculated by `semantic_risk_analysis` by looking for negative sentiment, threatening language, or personal grievance indicators.
        3. Adjusted Base Risk: Combines Base Risk with Contextual and Semantic Risk.
        4. Combination Multiplier: Calculated based on co-occurring high-risk entity types (defined in improve_risk_combo_detection). This multiplier increases the risk significantly if dangerous combinations are present (e.g., SUSPICIOUS_BEHAVIOR + SENSITIVE_INFO).
        5. Time Analysis: Uses `enhanced_time_analysis` to check if the tweet occurred during off-hours (weekends or outside 8 AM-5 PM) or if a TIME_ANOMALY entity was detected. Off-hours activity increases the risk_multiplier.
        6. Final Risk: Adjusted Base Risk * risk_multiplier.
      Outputs a risk_metrics dictionary containing all these values, plus entity counts, high-risk combo details, etc., for the tweet.
    """
    if risk_multipliers is None:
        risk_multipliers = improve_risk_combo_detection()

    assessed_tweets = []

    for tweet in risk_weighted_tweets:
        assessed_tweet = tweet.copy()
        entities = tweet["entities"]
        tweet_text = tweet.get("text", "")
        tweet_timestamp = tweet.get("timestamp")

        # Check if TIME_ANOMALY entity is present
        time_anomaly_present = any(entity["entity_type"] == "TIME_ANOMALY" for entity in entities)

        # Check for off-hours activity with explicit time anomaly detection
        is_off_hours = enhanced_time_analysis(tweet_timestamp, tweet_text, time_anomaly_present)

        context_risk, context_signals = context_aware_risk_assessment(tweet_text)
        semantic_risk, semantic_indicators = semantic_risk_analysis(tweet_text)

        if not entities:
            # Handle empty entities case
            assessed_tweet["risk_metrics"] = {
                "total_risk": (context_risk + semantic_risk) * 0.5,
                "base_risk": 0.0,
                "context_risk": context_risk,
                "semantic_risk": semantic_risk,
                "entity_type_risks": {},
                "high_risk_combinations": 0,
                "risk_density": 0.0,
                "is_precalculated": False,
                "context_signals": context_signals,
                "semantic_indicators": semantic_indicators,
                "off_hours": is_off_hours
            }
            assessed_tweets.append(assessed_tweet)
            continue

        base_risk = sum(entity["risk_score"] for entity in entities)
        entity_type_risks = {}
        entity_types_present = set()
        entity_type_counts = {}

        # Count entity types
        for entity in entities:
            entity_type = entity["entity_type"]
            entity_types_present.add(entity_type)
            entity_type_risks[entity_type] = entity_type_risks.get(entity_type, 0) + entity["risk_score"]
            entity_type_counts[entity_type] = entity_type_counts.get(entity_type, 0) + 1

        # Detect high-risk combinations
        high_risk_combinations = 0
        risk_multiplier = 1.0
        high_risk_combo_details = []

        # Check for combinations where both types exist
        for combo, multiplier in risk_multipliers.items():
            type1, type2 = combo

            if type1 == type2:
                # Same type combination (e.g. multiple SUSPICIOUS_BEHAVIOR)
                if type1 in entity_type_counts and entity_type_counts[type1] > 1:
                    additional_entities = entity_type_counts[type1] - 1
                    high_risk_combinations += additional_entities
                    risk_multiplier *= multiplier ** additional_entities
                    high_risk_combo_details.append(f"Multiple {type1}: {entity_type_counts[type1]}")
            else:
                # Different type combination
                if type1 in entity_types_present and type2 in entity_types_present:
                    # Count this as one combination regardless of how many of each type
                    high_risk_combinations += 1
                    risk_multiplier *= multiplier
                    high_risk_combo_details.append(f"{type1} + {type2}")

        # Apply off-hours multiplication if detected
        if is_off_hours:
            risk_multiplier *= 1.2  # 20% increase for off-hours activity

        # Integrate contextual and semantic risk
        adjusted_base_risk = base_risk + (context_risk * 0.7) + (semantic_risk * 0.7)

        final_risk = adjusted_base_risk * risk_multiplier
        risk_density = final_risk / max(len(entities), 1)

        risk_metrics = {
            "base_risk": base_risk,
            "adjusted_base_risk": adjusted_base_risk,
            "context_risk": context_risk,
            "semantic_risk": semantic_risk,
            "risk_multiplier": risk_multiplier,
            "total_risk": final_risk,
            "entity_type_risks": entity_type_risks,
            "high_risk_combinations": high_risk_combinations,
            "high_risk_combo_details": high_risk_combo_details,
            "entity_type_counts": entity_type_counts,
            "risk_density": risk_density,
            "is_precalculated": False,
            "context_signals": context_signals,
            "semantic_indicators": semantic_indicators,
            "off_hours": is_off_hours
        }

        # Add metadata if available
        if "metadata" in tweet and tweet["metadata"].get("classification"):
            risk_metrics["classification"] = tweet["metadata"]["classification"]

        if "metadata" in tweet and tweet["metadata"].get("sentiment"):
            risk_metrics["sentiment"] = tweet["metadata"]["sentiment"]

        assessed_tweet["risk_metrics"] = risk_metrics
        assessed_tweets.append(assessed_tweet)

    return assessed_tweets

# ============== 4. User-Level Profile Aggregation ==============
"""
1. Calls aggregate_user_profiles which groups the assessed tweets by user_id
2. For each user, it calculates aggregate statistics across all their tweets:
    total risk, average risk, max risk, standard deviation of risk,
    entity counts/density/distribution per type,
    high-risk combination counts/density,
    off-hours percentage,
    sentiment/classification distribution.
3. It also calls detect_behavior_changes to analyze temporal patterns like
   risk spikes or increasing trends, adding a temporal_risk score and anomaly flags.
"""

def detect_behavior_changes(user_tweets):
    """
    This function combines all detected anomalies into a composite temporal risk score:
      Temporal Risk = Σ(Spike Penalties) + Σ(Trend Penalties) + Σ(Off-Hours Penalties) + Σ(New Entity Penalties)

    Where:
      Each spike add 0.2 - mid range
      Each trend add 0.3 - most concerning
      Each off-hours add 0.25 - mid range
      Each new entity add 0.15 - for supporting evidence

    Args:
        user_tweets (list): List of tweets from a single user, with risk metrics

    Returns:
        tuple: (temporal_risk_score, anomalies) where temporal_risk_score is a float
               and anomalies is a list of detected anomalies
    """
    if len(user_tweets) < 3:
        return 0.0, []

    # Sort tweets by timestamp if available
    sorted_tweets = sorted(user_tweets,
                          key=lambda t: t.get("timestamp", ""),
                          reverse=False)

    risk_scores = [t.get("risk_metrics", {}).get("total_risk", 0) for t in sorted_tweets]
    anomalies = []
    temporal_risk = 0.0

    # Identifies sudden jumps in risk that may indicate escalation of suspicious activity.
    # Compares each tweet's risk score to its immediate predecessor
    # Triggers a spike detection when:
    #     Current risk > 2 × previous risk (100% increase)
    #     Current risk > 0.5 (absolute threshold)
    # Adds 0.2 to the temporal risk score for each spike
    for i in range(1, len(risk_scores)):
        if i > 0 and risk_scores[i] > 2 * risk_scores[i-1] and risk_scores[i] > 0.5:
            anomalies.append(f"RISK_SPIKE_AT_{i}")
            temporal_risk += 0.2

    # Catches gradual escalation patterns that might be missed by spike detection.
    # Divides the timeline into thirds
    # Compares average risk of first 1/3 vs last 1/3
    # Trend detected when:
    #     Last third average > 1.5 × first third average (50% increase)
    #     Last third average > 0.4 (absolute threshold)
    # Adds 0.3 to temporal risk score
    if len(risk_scores) >= 3:
        first_third = sum(risk_scores[:len(risk_scores)//3]) / (len(risk_scores)//3)
        last_third = sum(risk_scores[-len(risk_scores)//3:]) / (len(risk_scores)//3)

        if last_third > 1.5 * first_third and last_third > 0.4:
            anomalies.append("INCREASING_RISK_TREND")
            temporal_risk += 0.3

    # Check for off-hours pattern change - detects unusual timing patterns that correlate with insider threats.
    # Calculates proportion of off-hours tweets:
    #   off_hours_count / total_tweets
    # Flags when:
    #   ≥70% of activity is off-hours
    #   Minimum 3 tweets for statistical significance
    # Adds 0.25 to temporal risk
    off_hours_count = [t.get("risk_metrics", {}).get("off_hours", False) for t in sorted_tweets].count(True)
    if off_hours_count >= len(sorted_tweets) * 0.7 and len(sorted_tweets) >= 3:
        anomalies.append("PREDOMINANTLY_OFF_HOURS")
        temporal_risk += 0.25

    # Check for sudden appearance of high-risk entity types
    entity_types_by_tweet = []
    for tweet in sorted_tweets:
        risk_metrics = tweet.get("risk_metrics", {})
        entity_types = set(risk_metrics.get("entity_type_counts", {}).keys())
        entity_types_by_tweet.append(entity_types)

    high_risk_types = {"SUSPICIOUS_BEHAVIOR", "SENSITIVE_INFO", "TIME_ANOMALY"}

    # Check for sudden appearance of high-risk entity types, effecively detects
    # when users start discussing new categories of sensitive/sus topics.
    # Tracks appearance of new high-risk entity types (e.g, SUSPICIOUS_BEHAVIOR, SENSITIVE_INFO, TIME_ANOMALY)
    # Flags when >= 2 new high risk types that appear compared to previous tweet
    # Add 0.15 to temporal risk per occurrence
    for i in range(1, len(entity_types_by_tweet)):
        new_high_risk = high_risk_types.intersection(entity_types_by_tweet[i]) - \
                       high_risk_types.intersection(entity_types_by_tweet[i-1])
        if len(new_high_risk) >= 2:
            anomalies.append(f"NEW_HIGH_RISK_ENTITIES_AT_{i}")
            temporal_risk += 0.15



    return temporal_risk, anomalies

def aggregate_user_profiles(assessed_tweets):
    """
    Transforms tweet-level risk signals into holistic user-level profiles
    This multi-dimensional approach captures risk patterns, volatility,
    and distribution across entity types, providing a
    temporal and behavioral framework that aligns with insider threat psychology
    models focused on prolonged pattern detection rather than isolated incidents.
    """
    # Group tweets by user
    tweets_by_user = defaultdict(list)
    for tweet in assessed_tweets:
        user_id = tweet["user_id"]
        tweets_by_user[user_id].append(tweet)

    # Analyze each user profile
    user_profiles = {}
    for user_id, user_tweets in tweets_by_user.items():
        # Detect behavior changes over time
        temporal_risk, anomalies = detect_behavior_changes(user_tweets)

        profile = {
            "tweets": user_tweets,
            "tweet_count": len(user_tweets),
            "total_risk": 0.0,
            "entity_counts": defaultdict(int),
            "entity_risks": defaultdict(float),
            "high_risk_combinations": 0,
            "high_risk_combo_details": [],
            "max_tweet_risk": 0.0,
            "classifications": defaultdict(int),
            "sentiments": defaultdict(int),
            "off_hours_count": 0,
            "temporal_risk": temporal_risk,
            "behavior_anomalies": anomalies,
            "context_signals": defaultdict(int),
            "semantic_indicators": defaultdict(int)
        }

        for tweet in user_tweets:
            risk_metrics = tweet.get("risk_metrics", {})

            profile["total_risk"] += risk_metrics.get("total_risk", 0)

            # Count high risk combinations
            high_risk_count = risk_metrics.get("high_risk_combinations", 0)
            if high_risk_count > 0:
                profile["high_risk_combinations"] += high_risk_count
                profile["high_risk_combo_details"].extend(risk_metrics.get("high_risk_combo_details", []))

            profile["max_tweet_risk"] = max(
                profile["max_tweet_risk"],
                risk_metrics.get("total_risk", 0)
            )

            # Track entity counts and risks
            for entity_type, count in risk_metrics.get("entity_type_counts", {}).items():
                profile["entity_counts"][entity_type] += count

            for entity_type, risk in risk_metrics.get("entity_type_risks", {}).items():
                profile["entity_risks"][entity_type] += risk

            # Track classifications and sentiments
            if "classification" in risk_metrics:
                classification = risk_metrics["classification"]
                profile["classifications"][classification] += 1

            if "sentiment" in risk_metrics:
                sentiment = risk_metrics["sentiment"]
                profile["sentiments"][sentiment] += 1

            # Count off-hours activity - crucial for off_hours_percentage
            if risk_metrics.get("off_hours", False):
                profile["off_hours_count"] += 1

            # Aggregate context signals
            for signal in risk_metrics.get("context_signals", []):
                profile["context_signals"][signal] += 1

            # Aggregate semantic indicators
            for indicator in risk_metrics.get("semantic_indicators", []):
                profile["semantic_indicators"][indicator] += 1

        tweet_count = profile["tweet_count"]

        if tweet_count > 0:
            profile["avg_risk_per_tweet"] = profile["total_risk"] / tweet_count

            total_entities = sum(profile["entity_counts"].values())
            profile["total_entity_count"] = total_entities

            if total_entities > 0:
                profile["entity_density"] = total_entities / tweet_count
                profile["entity_distribution"] = {
                    entity_type: (count / total_entities) * 100
                    for entity_type, count in profile["entity_counts"].items()
                }
            else:
                profile["entity_density"] = 0
                profile["entity_distribution"] = {}

            # Calculate high_risk_combo_density - key metric we need to fix
            profile["high_risk_combo_density"] = profile["high_risk_combinations"] / tweet_count

            # Track classification distributions
            total_classifications = sum(profile["classifications"].values())
            if total_classifications > 0:
                profile["classification_distribution"] = {
                    cls: (count / total_classifications) * 100
                    for cls, count in profile["classifications"].items()
                }
            else:
                profile["classification_distribution"] = {}

            # Track sentiment distributions
            total_sentiments = sum(profile["sentiments"].values())
            if total_sentiments > 0:
                profile["sentiment_distribution"] = {
                    sentiment: (count / total_sentiments) * 100
                    for sentiment, count in profile["sentiments"].items()
                }
            else:
                profile["sentiment_distribution"] = {}

            # Calculate off_hours_percentage - key metric we need to fix
            profile["off_hours_percentage"] = (profile["off_hours_count"] / tweet_count) * 100

            # Calculate frequency of context signals and semantic indicators
            profile["context_signal_frequency"] = {
                signal: (count / tweet_count) * 100
                for signal, count in profile["context_signals"].items()
            }

            profile["semantic_indicator_frequency"] = {
                indicator: (count / tweet_count) * 100
                for indicator, count in profile["semantic_indicators"].items()
            }

            # Calculate risk statistics if we have enough data
            if tweet_count > 1:
                risk_values = [t.get("risk_metrics", {}).get("total_risk", 0) for t in profile["tweets"]]
                profile["risk_std_dev"] = np.std(risk_values)

                # Calculate risk trend (positive means increasing risk)
                if len(risk_values) >= 2:
                    first_half = sum(risk_values[:len(risk_values)//2]) / max(len(risk_values)//2, 1)
                    second_half = sum(risk_values[len(risk_values)//2:]) / max(len(risk_values) - len(risk_values)//2, 1)
                    profile["risk_trend"] = second_half - first_half
                else:
                    profile["risk_trend"] = 0.0
            else:
                profile["risk_std_dev"] = 0.0
                profile["risk_trend"] = 0.0
        else:
            # Default values for empty profiles
            profile["avg_risk_per_tweet"] = 0.0
            profile["total_entity_count"] = 0
            profile["entity_density"] = 0.0
            profile["entity_distribution"] = {}
            profile["high_risk_combo_density"] = 0.0
            profile["classification_distribution"] = {}
            profile["sentiment_distribution"] = {}
            profile["off_hours_percentage"] = 0.0
            profile["risk_std_dev"] = 0.0
            profile["risk_trend"] = 0.0
            profile["context_signal_frequency"] = {}
            profile["semantic_indicator_frequency"] = {}

        # Convert defaultdicts to regular dicts for serialization
        profile["entity_counts"] = dict(profile["entity_counts"])
        profile["entity_risks"] = dict(profile["entity_risks"])
        profile["classifications"] = dict(profile["classifications"])
        profile["sentiments"] = dict(profile["sentiments"])
        profile["context_signals"] = dict(profile["context_signals"])
        profile["semantic_indicators"] = dict(profile["semantic_indicators"])

        # Remove duplicate high risk combination details
        if profile["high_risk_combo_details"]:
            profile["high_risk_combo_details"] = list(set(profile["high_risk_combo_details"]))

        user_profiles[user_id] = profile

    return user_profiles

# ============== 5. Feature Vector Construction ==============
"""
Calls construct_feature_vectors to convert each user's aggregated profile dictionary
into a fixed-length numerical feature vector to ensures:
    All user profiles are represented with the same dimensions/features
    Features are properly scaled and normalized
    Missing values are handled consistently
The raw profile dictionaries contain:
    Variable numbers of entities
    Different combinations of risk indicators
    Nested structures (counts, percentages, distributions)
The feature vector:
    Flattens this into a fixed set of comparable metrics
    Selects only the most informative risk indicators
    Maintains the key signals while discarding noise
Enables direct comparison between users by:
    Creating common reference points (same features in same positions)
    Allowing distance/similarity calculations between vectors
    Supporting clustering and outlier detection algorithms

A user's raw profile might contain:
  {
    "total_risk": 45.2,
    "entity_counts": {"SUSPICIOUS_BEHAVIOR": 3, "SENSITIVE_INFO": 2},
    "off_hours_percentage": 35.7
    # ... 50+ other metrics
  }
Gets converted to a fixed vector like:
  [45.2, 0.32, 0.21, 0.357, ...]  # 100+ dimensions
"""


def construct_user_profile_features(user_profiles, entity_risk_weights=None):
    """
    Translates multi-dimensional user risk profiles into fixed-length numerical
    feature vectors and returns the list of feature names.

    Args:
        user_profiles (dict): Dictionary of user profiles.
        entity_risk_weights (dict, optional): Weights for entity types.

    Returns:
        tuple: A tuple containing:
            - feature_vectors (dict): Dictionary mapping user IDs to feature vectors (value includes vector, names, dict).
            - feature_names (list): List of feature names used in the vectors.
    """
    if entity_risk_weights is None:
        # Default weights (keep your original logic or update as needed)
        entity_risk_weights = {
            "SUSPICIOUS_BEHAVIOR": 0.9, "SENSITIVE_INFO": 0.8, "TIME_ANOMALY": 0.7,
            "TECH_ASSET": 0.6, "MEDICAL_CONDITION": 0.4, "SENTIMENT_INDICATOR": 0.3,
            "PERSON": 0.2, "ORG": 0.2, "LOC": 0.1
        } #

    feature_vectors = {}
    all_entity_types = set(entity_risk_weights.keys())
    all_classifications = set()
    all_sentiments = set()
    final_feature_names = [] # Variable to store the feature names

    # First pass to gather all possible classifications and sentiments
    # This ensures feature names are consistent even if some users lack certain categories
    for profile in user_profiles.values():
        all_classifications.update(profile.get("classifications", {}).keys())
        all_sentiments.update(profile.get("sentiments", {}).keys())

    # Determine the full set of feature names *before* iterating through users
    # This guarantees consistency
    example_features = {}
    example_features.update({
        "total_risk_score": 0.0, "avg_risk_per_tweet": 0.0, "max_tweet_risk": 0.0,
        "risk_std_dev": 0.0, "entity_density": 0.0, "high_risk_combo_density": 0.0,
        "off_hours_percentage": 0.0
    }) #
    for entity_type in all_entity_types:
        example_features[f"{entity_type.lower()}_count"] = 0
        example_features[f"{entity_type.lower()}_per_tweet"] = 0.0
        example_features[f"{entity_type.lower()}_risk"] = 0.0
        example_features[f"{entity_type.lower()}_risk_per_tweet"] = 0.0
        example_features[f"{entity_type.lower()}_percentage"] = 0.0 #
    for classification in all_classifications:
        example_features[f"classification_{classification.lower()}_count"] = 0
        example_features[f"classification_{classification.lower()}_percentage"] = 0.0 #
    for sentiment in all_sentiments:
        example_features[f"sentiment_{sentiment.lower()}_count"] = 0
        example_features[f"sentiment_{sentiment.lower()}_percentage"] = 0.0 #

    final_feature_names = sorted(example_features.keys()) # Get the canonical feature names

    # Now process each user profile using the consistent feature names
    for user_id, profile in user_profiles.items():
        tweet_count = profile.get("tweet_count", len(profile.get("tweets", [])))

        # Initialize features dict with zeros based on final_feature_names
        features = {name: 0.0 for name in final_feature_names} # Use 0.0 as default

        if tweet_count == 0 and user_id == "dummy_user": # Special handling for dummy user if needed
             # For the dummy user case, just return the empty feature vector structure
             feature_vector = np.array([features[name] for name in final_feature_names])
             feature_vectors[user_id] = {
                 "feature_vector": feature_vector,
                 "feature_names": final_feature_names,
                 "feature_dict": features
             }
             continue # Skip rest of processing for dummy user


        # Recalculate features for the actual user, ensuring all keys exist
        features["total_risk_score"]= profile.get("total_risk", 0.0) # Use .get for safety
        features["avg_risk_per_tweet"]= profile.get("avg_risk_per_tweet", 0.0)
        features["max_tweet_risk"]= profile.get("max_tweet_risk", 0.0)
        features["risk_std_dev"]= profile.get("risk_std_dev", 0.0)
        features["entity_density"]= profile.get("entity_density", 0.0)
        features["high_risk_combo_density"]= profile.get("high_risk_combo_density", 0.0)
        features["off_hours_percentage"]= profile.get("off_hours_percentage", 0.0) #

        for entity_type in all_entity_types:
            count = profile.get("entity_counts", {}).get(entity_type, 0)
            risk = profile.get("entity_risks", {}).get(entity_type, 0.0)
            percentage = profile.get("entity_distribution", {}).get(entity_type, 0.0)

            # Check if keys exist before assigning
            if f"{entity_type.lower()}_count" in features:
                features[f"{entity_type.lower()}_count"] = count
            if f"{entity_type.lower()}_per_tweet" in features:
                 features[f"{entity_type.lower()}_per_tweet"] = count / tweet_count if tweet_count > 0 else 0.0
            if f"{entity_type.lower()}_risk" in features:
                 features[f"{entity_type.lower()}_risk"] = risk
            if f"{entity_type.lower()}_risk_per_tweet" in features:
                 features[f"{entity_type.lower()}_risk_per_tweet"] = risk / tweet_count if tweet_count > 0 else 0.0
            if f"{entity_type.lower()}_percentage" in features:
                 features[f"{entity_type.lower()}_percentage"] = percentage #

        for classification in all_classifications:
            count = profile.get("classifications", {}).get(classification, 0)
            percentage = profile.get("classification_distribution", {}).get(classification, 0.0)
            if f"classification_{classification.lower()}_count" in features:
                 features[f"classification_{classification.lower()}_count"] = count
            if f"classification_{classification.lower()}_percentage" in features:
                 features[f"classification_{classification.lower()}_percentage"] = percentage #

        for sentiment in all_sentiments:
            count = profile.get("sentiments", {}).get(sentiment, 0)
            percentage = profile.get("sentiment_distribution", {}).get(sentiment, 0.0)
            if f"sentiment_{sentiment.lower()}_count" in features:
                 features[f"sentiment_{sentiment.lower()}_count"] = count
            if f"sentiment_{sentiment.lower()}_percentage" in features:
                 features[f"sentiment_{sentiment.lower()}_percentage"] = percentage #


        # Create the feature vector using the canonical order
        feature_vector = np.array([features[name] for name in final_feature_names])

        feature_vectors[user_id] = {
            "feature_vector": feature_vector,
            "feature_names": final_feature_names, # Use the consistent names
            "feature_dict": features
        }

    # Return both the dictionary and the consistent list of feature names
    # Handle the case where user_profiles might be empty (e.g., initial dummy call)
    if not final_feature_names and user_profiles:
         # If somehow names weren't set but profiles exist, grab from first profile as fallback
         first_user_id = list(user_profiles.keys())[0]
         if first_user_id in feature_vectors:
              final_feature_names = feature_vectors[first_user_id]["feature_names"]


    return feature_vectors, final_feature_names


ALL_ENTITY_TYPES = sorted([
    "SUSPICIOUS_BEHAVIOR", "SENSITIVE_INFO", "TIME_ANOMALY", "TECH_ASSET",
    "MEDICAL_CONDITION", "SENTIMENT_INDICATOR", "PERSON", "ORG", "LOC"
])
# Derived from context_aware_risk_assessment() signals
ALL_CONTEXT_SIGNALS = sorted([
    "SECURITY_CONTEXT", "SECRECY_INDICATORS", "FINANCIAL_MOTIVATION",
    "SECURITY_WITH_SECRECY"
])
# Derived from semantic_risk_analysis() indicators
ALL_SEMANTIC_INDICATORS = sorted([
    "NEGATIVE_SENTIMENT", "THREATENING_LANGUAGE", "PERSONAL_GRIEVANCE",
    "PERSONAL_NEGATIVITY", "PERSONAL_THREAT"
])


def extract_hybrid_tweet_features(
    risk_metrics: Dict[str, Any],
    all_entity_types: List[str],
    all_context_signals: List[str],
    all_semantic_indicators: List[str]
) -> Tuple[np.ndarray, List[str]]:
    """
    Extracts a feature vector for a single tweet based on its risk_metrics,
    implementing the hybrid approach (using rule scores as features).

    Args:
        risk_metrics (Dict[str, Any]): The output of assess_tweet_risk for one tweet.
        all_entity_types (List[str]): Master list of all possible entity types.
        all_context_signals (List[str]): Master list of all possible context signals.
        all_semantic_indicators (List[str]): Master list of all possible semantic indicators.

    Returns:
        Tuple[np.ndarray, List[str]]: A tuple containing:
            - feature_vector (np.ndarray): The numerical feature vector for the tweet.
            - feature_names (List[str]): The names corresponding to the vector elements.
    """
    # Use a temporary dictionary to build features, ensuring order via feature_names later
    features = {}
    feature_names = [] # Keep track of order

    # 1. Core rule-based scores from assess_tweet_risk
    core_scores = [
        'total_risk', 'base_risk', 'adjusted_base_risk', 'context_risk',
        'semantic_risk', 'risk_multiplier', 'high_risk_combinations', 'risk_density'
    ]
    for score_name in core_scores:
        features[score_name] = risk_metrics.get(score_name, 0.0) # Default to 0.0 if missing
        feature_names.append(score_name)

    # 2. Off-hours flag from assess_tweet_risk
    features['off_hours'] = 1.0 if risk_metrics.get('off_hours', False) else 0.0
    feature_names.append('off_hours')

    # 3. Entity type counts from assess_tweet_risk
    entity_counts = risk_metrics.get('entity_type_counts', {})
    for entity_type in all_entity_types:
        key = f"count_{entity_type.lower()}"
        features[key] = float(entity_counts.get(entity_type, 0)) # Use float for consistency
        feature_names.append(key)

    # 4. Context signal presence (binary) from assess_tweet_risk
    context_signals_present = set(risk_metrics.get('context_signals', []))
    for signal in all_context_signals:
        key = f"signal_{signal.lower()}"
        features[key] = 1.0 if signal in context_signals_present else 0.0
        feature_names.append(key)

    # 5. Semantic indicator presence (binary) from assess_tweet_risk
    semantic_indicators_present = set(risk_metrics.get('semantic_indicators', []))
    for indicator in all_semantic_indicators:
        key = f"indicator_{indicator.lower()}"
        features[key] = 1.0 if indicator in semantic_indicators_present else 0.0
        feature_names.append(key)

    # Create the final vector in the defined order
    feature_vector = np.array([features[name] for name in feature_names], dtype=float)

    return feature_vector, feature_names

# ============== 6. Normalization and Outlier Detection ==============
"""
Calls normalize_features (e.g., using min-max or z-score scaling)
to bring all features to a comparable scale

Calls detect_outliers to calculate a score indicating
how much a user's profile deviates from the norm, potentially flagging anomalous users
"""

def normalize_features(feature_vectors, method="minmax"):
    """
    Applies statistical normalization techniques to harmonize heterogeneous
    feature scales, critical for equitable comparison across different risk
    dimensions. The implementation supports multiple normalization strategies
    to accommodate varying distribution characteristics, ensuring that all signals
    contribute proportionally to downstream risk assessments regardless of
    their natural scales.
    """
    normalized_vectors = {}
    user_ids = list(feature_vectors.keys())

    if not user_ids:
        return {}

    feature_names = feature_vectors[user_ids[0]]["feature_names"]
    all_vectors = np.vstack([feature_vectors[uid]["feature_vector"] for uid in user_ids])

    if method == "minmax":
        min_vals = np.min(all_vectors, axis=0)
        max_vals = np.max(all_vectors, axis=0)
        range_vals = np.maximum(max_vals - min_vals, 1e-10) # Prevents division by zero for constant features
        normalized = (all_vectors - min_vals) / range_vals
    elif method == "zscore":
        mean_vals = np.mean(all_vectors, axis=0)
        std_vals = np.std(all_vectors, axis=0)
        std_vals = np.maximum(std_vals, 1e-10)
        normalized = (all_vectors - mean_vals) / std_vals
    else:
        raise ValueError(f"Unsupported normalization method: {method}")

    for i, user_id in enumerate(user_ids):
        normalized_vectors[user_id] = {
            "feature_vector": normalized[i],
            "feature_names": feature_names,
            "feature_dict": {
                name: normalized[i, j] for j, name in enumerate(feature_names)
            }
        }

    return normalized_vectors

def detect_outliers(feature_vectors, threshold=2.0, method="zscore"):
    """
    Identifies statistically anomalous users through principled outlier detection
    methodologies. The approach aligns with insider threat theory that emphasizes
    deviation from baseline behaviors as key risk indicators, rather than absolute
    risk levels. This leverages the statistical properties of normalized features
    to highlight users whose patterns diverge significantly from the population.
    """
    if method == "zscore" and list(feature_vectors.values())[0]["feature_vector"].max() <= 1.0:
        normalized = normalize_features(feature_vectors, method="zscore")
    else:
        normalized = feature_vectors

    outlier_scores = {}

    for user_id, vector_data in normalized.items():
        feature_vector = vector_data["feature_vector"]

        if method == "zscore":
            # For each feature, calculate how many "Standard Deviations" away a user is and take their most extreme deviation across all freatures.
            # Essentially catches users who are extreme in any single dimension
            # (e.g, someone who accesses WAY more files than others)
            outlier_score = np.max(np.abs(feature_vector))
        elif method == "percentile":
            # Calculate how many features are in the extreme range then calculate what percentage of their features are unusual.
            # Essentially catches users with multiple slightly unusual behaviors.
            # (Someone who's a bit unusual in several small ways that add up)
            outlier_score = np.sum(feature_vector > threshold) / len(feature_vector)
        else:
            raise ValueError(f"Unsupported outlier detection method: {method}")

        outlier_scores[user_id] = outlier_score

    return outlier_scores

# ============== 7. User Analysis Functions ==============

def create_dataframe(user_profiles, feature_vectors, normalized_vectors):
    all_user_data = []

    key_metrics = [
        "total_risk_score",
        "avg_risk_per_tweet",
        "max_tweet_risk",
        "risk_std_dev",
        "entity_density",
        "high_risk_combo_density",
        "off_hours_percentage"
    ]

    key_entity_types = [
        "SUSPICIOUS_BEHAVIOR",
        "SENSITIVE_INFO",
        "TIME_ANOMALY",
        "TECH_ASSET",
        "SENTIMENT_INDICATOR"
    ]

    for user_id, profile in user_profiles.items():
        if user_id not in feature_vectors:
            continue

        user_data = {"user_id": user_id}
        user_data["tweet_count"] = profile.get("tweet_count", 0)

        outlier_score = detect_outliers({user_id: normalized_vectors[user_id]}).get(user_id, 0)
        user_data["outlier_score"] = outlier_score

        for metric in key_metrics:
            if metric in feature_vectors[user_id]["feature_dict"]:
                user_data[metric] = feature_vectors[user_id]["feature_dict"][metric]
            else:
                user_data[metric] = 0.0

        for entity_type in key_entity_types:
            entity_count_key = entity_type.lower() + "_count"
            if entity_count_key in feature_vectors[user_id]["feature_dict"]:
                user_data[entity_count_key] = feature_vectors[user_id]["feature_dict"][entity_count_key]
            else:
                user_data[entity_count_key] = 0

            entity_per_tweet_key = entity_type.lower() + "_per_tweet"
            if entity_per_tweet_key in feature_vectors[user_id]["feature_dict"]:
                user_data[entity_per_tweet_key] = feature_vectors[user_id]["feature_dict"][entity_per_tweet_key]
            else:
                user_data[entity_per_tweet_key] = 0.0

            entity_risk_key = entity_type.lower() + "_risk"
            if entity_risk_key in feature_vectors[user_id]["feature_dict"]:
                user_data[entity_risk_key] = feature_vectors[user_id]["feature_dict"][entity_risk_key]
            else:
                user_data[entity_risk_key] = 0.0

        classifications = profile.get("classifications", {})
        if classifications:
            most_common = max(classifications.items(), key=lambda x: x[1])
            user_data["primary_classification"] = most_common[0]
            user_data["primary_classification_count"] = most_common[1]

        sentiments = profile.get("sentiments", {})
        if sentiments:
            most_common = max(sentiments.items(), key=lambda x: x[1])
            user_data["primary_sentiment"] = most_common[0]
            user_data["primary_sentiment_count"] = most_common[1]

        all_user_data.append(user_data)

    df = pd.DataFrame(all_user_data)

    if "total_risk_score" in df.columns:
        df = df.sort_values("total_risk_score", ascending=False)

    return df

def visualize_all_users_heatmap(user_df, save_path=None):
    """
    Implements a multi-dimensional visual comparison framework through heatmap
    visualization, facilitating intuitive pattern recognition across the entire
    user population. This design addresses the cognitive challenge of comparing
    numerous risk dimensions simultaneously, using color intensity to highlight
    relative risk positions and reveal correlation patterns between different
    risk indicators.
    """
    key_visualization_metrics = [
        "total_risk_score",
        "avg_risk_per_tweet",
        "max_tweet_risk",
        "entity_density",
        "suspicious_behavior_per_tweet",
        "sensitive_info_per_tweet",
        "time_anomaly_per_tweet",
        "tech_asset_per_tweet",
        "high_risk_combo_density",
        "off_hours_percentage"
    ]

    existing_metrics = [m for m in key_visualization_metrics if m in user_df.columns]

    max_users = 50
    if len(user_df) > max_users:
        display_df = user_df.head(max_users)
        print(f"Note: Displaying top {max_users} users by risk score out of {len(user_df)} total users")
    else:
        display_df = user_df

    user_ids = display_df["user_id"].tolist()
    data = display_df[existing_metrics].values

    normalized_data = np.zeros_like(data, dtype=float)
    for i in range(data.shape[1]):
        col_min = data[:, i].min()
        col_max = data[:, i].max()
        if col_max > col_min:
            normalized_data[:, i] = (data[:, i] - col_min) / (col_max - col_min)
        else:
            normalized_data[:, i] = 0

    plt.figure(figsize=(12, max(8, len(display_df) / 3)))

    metric_labels = [m.replace('_', ' ').title().replace('Per Tweet', '/Tweet') for m in existing_metrics]

    ax = sns.heatmap(normalized_data,
                     annot=False,
                     cmap="YlOrRd",
                     xticklabels=metric_labels,
                     yticklabels=user_ids,
                     cbar_kws={'label': 'Relative Value'})

    plt.title("User Risk Profile Comparison - All Metrics", fontsize=16)
    plt.xlabel("Risk Metrics", fontsize=12)
    plt.ylabel("User ID", fontsize=12)
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
    else:
        plt.show()

    plt.figure(figsize=(12, max(8, len(display_df) / 3)))

    risk_df = display_df.sort_values("total_risk_score", ascending=True)

    bars = plt.barh(risk_df["user_id"], risk_df["total_risk_score"])

    for bar in bars:
        width = bar.get_width()
        plt.text(width + 0.5, bar.get_y() + bar.get_height()/2,
                f"{width:.1f}", ha='left', va='center')

    plt.title("Total Risk Score by User", fontsize=16)
    plt.xlabel("Risk Score", fontsize=12)
    plt.ylabel("User ID", fontsize=12)
    plt.tight_layout()

    if save_path:
        risk_path = save_path.replace('.png', '_risk_scores.png')
        plt.savefig(risk_path, dpi=300, bbox_inches='tight')
        plt.close()
    else:
        plt.show()

def visualize_user_metrics_parallel(user_df, save_path=None):
    """
    Leverages parallel coordinates visualization to reveal multi-dimensional
    relationships between risk metrics across users. This technique facilitates
    the discovery of risk archetypes and correlated behaviors that might indicate
    different insider threat profiles, supporting a more nuanced understanding
    of risk patterns beyond simplistic threshold-based approaches.
    """
    key_visualization_metrics = [
        "total_risk_score",
        "avg_risk_per_tweet",
        "entity_density",
        "suspicious_behavior_per_tweet",
        "sensitive_info_per_tweet",
        "high_risk_combo_density"
    ]

    existing_metrics = [m for m in key_visualization_metrics if m in user_df.columns]

    max_users = 20
    if len(user_df) > max_users:
        display_df = user_df.head(max_users).copy()
        print(f"Note: Displaying top {max_users} users by risk score in parallel coordinates plot")
    else:
        display_df = user_df.copy()

    display_df['risk_class'] = pd.qcut(display_df['total_risk_score'],
                                      min(5, len(display_df)),
                                      labels=False)

    plot_df = display_df.copy()
    for col in existing_metrics:
        if plot_df[col].max() > plot_df[col].min():
            plot_df[col] = (plot_df[col] - plot_df[col].min()) / (plot_df[col].max() - plot_df[col].min())

    plt.figure(figsize=(12, 8))

    pd.plotting.parallel_coordinates(plot_df, 'risk_class', cols=existing_metrics, colormap='YlOrRd')

    plt.title("User Risk Metrics - Parallel Coordinates", fontsize=16)
    plt.xticks(rotation=45, ha='right')
    plt.ylabel("Normalized Value", fontsize=12)
    plt.grid(True, linestyle='--', alpha=0.7)
    plt.legend(title='Risk Level', loc='upper right', title_fontsize=12)
    plt.tight_layout()

    if save_path:
        parallel_path = save_path.replace('.png', '_parallel.png')
        plt.savefig(parallel_path, dpi=300, bbox_inches='tight')
        plt.close()
    else:
        plt.show()

def create_risk_scatterplot_matrix(user_df, save_path=None):
    """
    Implements a correlation-focused visualization matrix to identify statistical
    relationships between different risk dimensions. This approach facilitates the
    discovery of co-occurring risk patterns and potential behavioral signatures
    that might indicate insider threats, supporting hypothesis generation about
    underlying psychological and behavioral factors driving risk.
    """
    key_visualization_metrics = [
        "total_risk_score",
        "avg_risk_per_tweet",
        "suspicious_behavior_per_tweet",
        "sensitive_info_per_tweet",
        "entity_density",
        "high_risk_combo_density"
    ]

    existing_metrics = [m for m in key_visualization_metrics if m in user_df.columns]

    max_users = 30
    if len(user_df) > max_users:
        display_df = user_df.head(max_users)
        print(f"Note: Using top {max_users} users by risk score for scatterplot matrix")
    else:
        display_df = user_df

    plt.figure(figsize=(14, 12))

    plot_df = display_df[existing_metrics].copy()
    plot_df.columns = [col.replace('_', ' ').title().replace('Per Tweet', '/Tweet')
                      for col in plot_df.columns]

    sns.pairplot(plot_df, height=2.5, aspect=1.2,
                plot_kws={'alpha': 0.7, 's': 80, 'edgecolor': 'k', 'linewidth': 0.5})

    plt.suptitle("Relationships Between Risk Metrics", fontsize=16, y=1.02)
    plt.tight_layout()

    if save_path:
        scatter_path = save_path.replace('.png', '_scattermatrix.png')
        plt.savefig(scatter_path, dpi=300, bbox_inches='tight')
        plt.close()
    else:
        plt.show()

def create_metrics_table(user_df, save_path=None):
    """
    Consolidates multi-dimensional user risk data into a structured tabular format
    optimized for detailed analysis and third-party integration. This approach
    balances human readability with computational accessibility, supporting both
    direct analysis by security teams and downstream integration with existing
    security information and event management (SIEM) systems.
    """
    export_df = user_df.copy()

    for col in export_df.columns:
        if col in ['user_id', 'primary_classification', 'primary_sentiment']:
            continue
        elif 'count' in col and export_df[col].dtype != 'object':
            export_df[col] = export_df[col].fillna(0).astype(int)
        elif export_df[col].dtype != 'object':
            export_df[col] = export_df[col].fillna(0).round(2)

    if save_path:
        csv_path = save_path.replace('.png', '_metrics.csv')
        export_df.to_csv(csv_path, index=False)
        print(f"Metrics table saved to {csv_path}")

    return export_df

def generate_visualizations(user_profiles, feature_vectors, normalized_vectors, output_dir):
    """
    Orchestrates the generation of complementary visualization modalities that
    collectively provide a holistic view of user risk across the entire population.
    This multi-faceted approach addresses different analytical needs from pattern
    discovery to detailed comparison, supporting both high-level risk triage and
    in-depth investigation of specific user behaviors and risk signatures.
    """
    viz_dir = os.path.join(output_dir, 'visualizations')
    os.makedirs(viz_dir, exist_ok=True)

    print("\nCreating user metrics dataframe...")
    user_df = create_dataframe(user_profiles, feature_vectors, normalized_vectors)

    print(f"Generated metrics for {len(user_df)} users")

    print("\nGenerating visualizations...")

    print("Creating heatmap visualization...")
    visualize_all_users_heatmap(user_df, os.path.join(viz_dir, "all_users_heatmap.png"))

    print("Creating parallel coordinates visualization...")
    visualize_user_metrics_parallel(user_df, os.path.join(viz_dir, "all_users_parallel.png"))

    print("Creating scatterplot matrix...")
    create_risk_scatterplot_matrix(user_df, os.path.join(viz_dir, "all_users_scattermatrix.png"))

    print("Exporting metrics table...")
    metrics_df = create_metrics_table(user_df, os.path.join(viz_dir, "all_users_metrics.png"))

    print(f"\nAll visualizations generated in {viz_dir}")

    return user_df

# ============== 8. Classification Model Training and Evaluation ==============

# def train_and_evaluate_classification(data_file_path: str) -> Optional[LogisticRegression]:
#     """
#     Trains a Logistic Regression model using the HYBRID approach.
#     Features include rule-based scores and tweet-level metrics from assess_tweet_risk.
#     Filters data for binary classification ('malicious'/'non-malicious').
#     Includes robust label extraction and diagnostic steps.

#     Args:
#         data_file_path (str): Path to the CSV data file containing tweets and ground truth classification.

#     Returns:
#         Optional[LogisticRegression]: Trained model, or None if errors occur.
#     """
#     print(f"\n--- Starting Classification Training (Hybrid Approach V2) for {data_file_path} ---")

#     # Step 1: Load and Filter Data
#     try:
#         data_loaded = load_production_data(data_file_path)
#         if isinstance(data_loaded, pd.DataFrame):
#             df = data_loaded
#         elif isinstance(data_loaded, list):
#              df = pd.DataFrame(data_loaded)
#         else:
#              print("Error: load_production_data returned unexpected type.")
#              return None

#         print(f"Original data size: {len(df)} rows")
#         allowed_labels = ['malicious', 'non-malicious']
#         mask = df['classification'].isin(allowed_labels)
#         df_filtered = df[mask].copy()
#         print(f"Filtered data size (malicious/non-malicious only): {len(df_filtered)} rows")

#         if len(df_filtered) == 0:
#             print("Error: No 'malicious' or 'non-malicious' samples found after filtering.")
#             return None

#         # Convert filtered data back to list of dicts for processing functions
#         data_for_processing = df_filtered.to_dict(orient='records')

#     except Exception as e:
#         print(f"Error during data loading or filtering: {e}")
#         return None

#     # Step 2: Run Risk Profiling Pipeline (up to assess_tweet_risk)
#     print("Running initial risk profiling steps to generate tweet metrics...")
#     try:
#         # Note: These functions need to be defined elsewhere in your script
#         standardized_tweets = extract_and_standardize_entities(data_for_processing)
#         risk_weighted_tweets = assign_risk_weights(standardized_tweets)
#         assessed_tweets = assess_tweet_risk(risk_weighted_tweets)
#         print(f"Processed {len(assessed_tweets)} tweets through risk assessment.")
#     except Exception as e:
#         print(f"Error during risk assessment pipeline: {e}")
#         return None

#     # Step 3: Extract Hybrid Features for each tweet
#     tweet_features_list = []
#     hybrid_feature_names = [] # Define outside loop

#     print("Extracting hybrid features for each tweet...")
#     try:
#         # Ensure these constants are defined globally or passed
#         global ALL_ENTITY_TYPES, ALL_CONTEXT_SIGNALS, ALL_SEMANTIC_INDICATORS

#         for i, tweet_data in enumerate(assessed_tweets):
#             risk_metrics = tweet_data.get("risk_metrics", {})
#             # Note: extract_hybrid_tweet_features needs to be defined elsewhere
#             feature_vector, current_feature_names = extract_hybrid_tweet_features(
#                 risk_metrics,
#                 ALL_ENTITY_TYPES,
#                 ALL_CONTEXT_SIGNALS,
#                 ALL_SEMANTIC_INDICATORS
#             )
#             tweet_features_list.append(feature_vector)
#             if i == 0: # Get feature names from the first tweet
#                 hybrid_feature_names = current_feature_names
#                 print(f"Determined {len(hybrid_feature_names)} hybrid features.")

#         if not hybrid_feature_names:
#              print("Error: Could not determine hybrid feature names.")
#              return None
#         if not tweet_features_list:
#              print("Error: No features were extracted.")
#              return None

#         X_features = np.vstack(tweet_features_list) # This will be used for X_train/X_test

#     except Exception as e:
#         print(f"Error during hybrid feature extraction: {e}")
#         return None

#     # Step 3.5: Extract Labels (Corrected Access)
#     print("Extracting labels...")
#     y_labels_list = []
#     try:
#         for i, tweet_data in enumerate(assessed_tweets):
#             # --- Corrected label access from previous fix ---
#             risk_metrics_dict = tweet_data.get('risk_metrics', {})
#             label = risk_metrics_dict.get('classification')
#             # --- End corrected label access ---

#             if label is None:
#                 print(f"CRITICAL Error: 'classification' key missing or 'risk_metrics' missing in assessed_tweets index {i}.")
#                 print(f"Tweet Data (missing key): {tweet_data}")
#                 raise KeyError(f"'classification' key missing in risk_metrics or risk_metrics missing in assessed_tweets index {i}. Check data processing pipeline.")
#             elif label not in allowed_labels:
#                  print(f"CRITICAL Error: Unexpected label '{label}' found at index {i} after filtering.")
#                  raise ValueError(f"Unexpected label '{label}' found at index {i} after filtering.")
#             y_labels_list.append(label)

#         y_labels = np.array(y_labels_list) # This will be used for y_train/y_test
#         print(f"Successfully extracted {len(y_labels)} labels.")

#     except KeyError as e:
#          print(f"Stopping execution due to missing key: {e}")
#          return None
#     except ValueError as e:
#          print(f"Stopping execution due to unexpected label value: {e}")
#          return None
#     except Exception as e:
#          print(f"An unexpected error occurred during label extraction: {e}")
#          return None

#     # Step 4: Split the Data
#     print("Splitting data into training and testing sets...")
#     # X_features and y_labels come from steps 3 and 3.5
#     X_train, X_test, y_train, y_test = train_test_split(
#         X_features, y_labels, test_size=0.2, random_state=42, stratify=y_labels
#     )
#     print(f"Training features shape: {X_train.shape}, Testing features shape: {X_test.shape}")

#     # Step 5: Scale Features & Train Model
#     print("Scaling features and training the classification model...")
#     model = LogisticRegression(solver='liblinear', random_state=42, max_iter=1000)
#     scaler = StandardScaler() # Define scaler here
#     try:
#         # Scale features
#         X_train_scaled = scaler.fit_transform(X_train)
#         X_test_scaled = scaler.transform(X_test)

#         # Train model
#         model.fit(X_train_scaled, y_train)
#         print("Model training complete (with scaled features).")

#         # --- Debug Step 1: Check Internal Class Order ---
#         print(f"DEBUG: Model classes order: {model.classes_}")
#         # --- End Debug Step 1 ---

#     except Exception as e:
#         print(f"Error during model fitting: {e}")
#         return None

#     # Step 6: Make predictions and Check Probabilities
#     y_pred = model.predict(X_test_scaled)
#     y_pred_proba = None # Initialize
#     y_pred_proba_all = None # Initialize
#     malicious_class_index = -1 # Initialize

#     try:
#         y_pred_proba_all = model.predict_proba(X_test_scaled) # Get probabilities for ALL classes

#         # --- Debug Step 2: Inspect Probabilities ---
#         print(f"DEBUG: Shape of predict_proba output: {y_pred_proba_all.shape}")
#         print(f"DEBUG: First 5 rows of predict_proba output:\n{y_pred_proba_all[:5]}")
#         # --- End Debug Step 2 ---

#         # Determine which column index corresponds to 'malicious' based on Step 1 output
#         # Check if model.classes_ is available (it should be after fitting)
#         if hasattr(model, 'classes_'):
#             classes_list = list(model.classes_)
#             if 'malicious' in classes_list:
#                  malicious_class_index = classes_list.index('malicious')
#                  print(f"DEBUG: Determined 'malicious' probability is in column index: {malicious_class_index}")
#                  y_pred_proba = y_pred_proba_all[:, malicious_class_index] # Select probability for 'malicious' class
#             else:
#                  print("CRITICAL Error: 'malicious' class not found in model.classes_")
#                  # Handle error appropriately, maybe return None or raise exception
#                  return None # Stop execution if positive class index cannot be found
#         else:
#              print("CRITICAL Error: model.classes_ not found after fitting.")
#              return None # Stop execution


#     except Exception as e:
#         y_pred_proba = None # Ensure it's None if error occurs
#         print(f"Could not get prediction probabilities: {e}")


#     # Step 7: Evaluate the model
#     print("\nClassification Model Evaluation (Hybrid Approach - Binary):")
#     accuracy = accuracy_score(y_test, y_pred)
#     precision = precision_score(y_test, y_pred, average='binary', pos_label='malicious', zero_division=0)
#     recall = recall_score(y_test, y_pred, average='binary', pos_label='malicious', zero_division=0)
#     f1 = f1_score(y_test, y_pred, average='binary', pos_label='malicious', zero_division=0)
#     class_labels = sorted(list(set(y_test) | set(y_pred)))
#     conf_matrix = confusion_matrix(y_test, y_pred, labels=class_labels)

#     print(f"  Accuracy: {accuracy:.4f}")
#     print(f"  Precision (for 'malicious'): {precision:.4f}")
#     print(f"  Recall (for 'malicious'): {recall:.4f}")
#     print(f"  F1-score (for 'malicious'): {f1:.4f}")
#     print(f"  Confusion Matrix (Labels: {class_labels}):\n{conf_matrix}")

#     # Calculate and Print ROC AUC (Corrected logic)
#     log_dict = { # Define dict outside try block
#         "classification_accuracy": accuracy,
#         "classification_precision_malicious": precision,
#         "classification_recall_malicious": recall,
#         "classification_f1_malicious": f1
#     }
#     if y_pred_proba is not None:
#         try:
#             # Ensure 'malicious' is treated as 1 for roc_auc_score
#             y_test_numeric = np.where(y_test == 'malicious', 1, 0)

#             # --- Debug Step 3: Verify Inputs to roc_auc_score ---
#             print(f"DEBUG: Shape of y_test_numeric: {y_test_numeric.shape}")
#             print(f"DEBUG: First 5 y_test_numeric: {y_test_numeric[:5]}")
#             print(f"DEBUG: Shape of y_pred_proba (for 'malicious'): {y_pred_proba.shape}")
#             print(f"DEBUG: First 5 y_pred_proba (for 'malicious'): {y_pred_proba[:5]}")
#             # --- End Debug Step 3 ---

#             roc_auc = roc_auc_score(y_test_numeric, y_pred_proba) # Compare true 0/1 labels with predicted prob of 1
#             log_dict["classification_roc_auc"] = roc_auc
#             print(f"  ROC AUC: {roc_auc:.4f}") # Print the corrected ROC AUC
#         except Exception as e_auc:
#              print(f"Could not calculate ROC AUC: {e_auc}")
#              log_dict["classification_roc_auc"] = None # Log as None if error occurs
#     else:
#         print("  ROC AUC: Not calculated (probabilities unavailable).")
#         log_dict["classification_roc_auc"] = None

#     # Log metrics to wandb (ensure wandb is initialized)
#     if wandb.run is not None:
#         try:
#             wandb.log(log_dict)
#         except Exception as e:
#             print(f"Could not log metrics to wandb: {e}")

#     # Optional: Feature Importance
#     if hasattr(model, 'coef_') and hybrid_feature_names: # Check if names exist
#         try:
#             feature_importance = pd.DataFrame({
#                 'feature': hybrid_feature_names,
#                 'coefficient': model.coef_[0]
#             })
#             feature_importance['abs_coefficient'] = feature_importance['coefficient'].abs()
#             feature_importance = feature_importance.sort_values('abs_coefficient', ascending=False)
#             print("\nTop 15 Feature Importances (Logistic Regression Coefficients on Scaled Data):")
#             print(feature_importance[['feature', 'coefficient']].head(15).to_string(index=False))
#             # Log feature importance table to wandb (optional)
#             # if wandb.run is not None:
#             #     try:
#             #         wandb.log({"feature_importances": wandb.Table(dataframe=feature_importance.head(20))})
#             #     except Exception as wb_err:
#             #         print(f"WandB Log Error (Feature Importances): {wb_err}")
#         except Exception as e:
#             print(f"Could not calculate or display feature importance: {e}")

#     # --- Diagnostic Step 1: Feature Correlation Analysis ---
#     print("\n--- Running Diagnostic: Feature Correlation Analysis ---")
#     try:
#         # Check if variables are available (they should be defined within this function scope)
#         if 'hybrid_feature_names' in locals() and 'X_train_scaled' in locals():
#             feature_df_corr = pd.DataFrame(X_train_scaled, columns=hybrid_feature_names)
#             corr_matrix = feature_df_corr.corr()

#             plt.figure(figsize=(15, 12))
#             sns.heatmap(corr_matrix, cmap='coolwarm', center=0, annot=False)
#             plt.title('Feature Correlation Matrix (Scaled Training Data)')
#             plt.xticks(rotation=90)
#             plt.yticks(rotation=0)
#             plt.tight_layout()

#             # Define path for saving the plot
#             correlation_plot_path = "/content/drive/MyDrive/cs491_DataAnalyzer/Dataset/insider_threat_ner_data/Result_From_NER/user_analysis/feature_correlation_heatmap.png"
#             # Ensure directory exists (optional, create if needed)
#             # os.makedirs(os.path.dirname(correlation_plot_path), exist_ok=True)
#             plt.savefig(correlation_plot_path, dpi=300)
#             print(f"Saved correlation heatmap to: {correlation_plot_path}")
#             plt.close() # Close figure to prevent display inline if not desired

#             # Log to WandB (optional)
#             if wandb.run is not None:
#                 try:
#                     wandb.log({"feature_correlation_heatmap": wandb.Image(correlation_plot_path)})
#                 except Exception as wb_err:
#                     print(f"WandB Log Error (Correlation Heatmap): {wb_err}")

#             # Print specific correlations
#             problem_features = ['count_suspicious_behavior', 'context_risk', 'risk_multiplier']
#             print("\nCorrelations of problematic features with all features (Top 10 absolute):")
#             for feature in problem_features:
#                 if feature in corr_matrix.columns:
#                     corrs = corr_matrix[feature].abs().sort_values(ascending=False)
#                     print(f"\nCorrelations for '{feature}':")
#                     print(corrs.head(11).to_string())
#                 else:
#                     print(f"Warning: Problematic feature '{feature}' not found in correlation matrix.")
#         else:
#             print("Warning: Could not run correlation analysis - required variables not found.")
#     except Exception as e:
#         print(f"Error during correlation analysis: {e}")
#     # --- End Diagnostic Step 1 ---

#     # --- Diagnostic Step 2: Univariate Feature Analysis (Box Plots) ---
#     print("\n--- Running Diagnostic: Univariate Feature Analysis (Box Plots) ---")
#     try:
#         # Check if variables are available
#         if 'hybrid_feature_names' in locals() and 'X_train' in locals() and 'y_train' in locals():
#             feature_df_unscaled = pd.DataFrame(X_train, columns=hybrid_feature_names)
#             feature_df_unscaled['label'] = y_train

#             problem_features = ['count_suspicious_behavior', 'context_risk', 'risk_multiplier']
#             plot_paths_univariate = [] # To store paths for WandB logging

#             print("\nGenerating box plots for problematic features vs. label:")
#             for feature in problem_features:
#                 if feature in feature_df_unscaled.columns:
#                     plt.figure(figsize=(8, 6))
#                     sns.boxplot(x='label', y=feature, data=feature_df_unscaled, order=['non-malicious', 'malicious'])
#                     plt.title(f'Distribution of {feature} by Label (Unscaled Train Data)')
#                     plt.xlabel("True Label")
#                     plt.ylabel(feature)
#                     plt.tight_layout()

#                     # Define path for saving the plot
#                     plot_path = f"/content/drive/MyDrive/cs491_DataAnalyzer/Dataset/insider_threat_ner_data/Result_From_NER/user_analysis/univariate_{feature}_boxplot.png"
#                     # Ensure directory exists (optional, create if needed)
#                     # os.makedirs(os.path.dirname(plot_path), exist_ok=True)
#                     plt.savefig(plot_path, dpi=300)
#                     print(f"Saved box plot for '{feature}' to: {plot_path}")
#                     plt.close() # Close figure
#                     plot_paths_univariate.append(plot_path)
#                 else:
#                     print(f"Warning: Problematic feature '{feature}' not found in unscaled data.")

#             # Log plots to WandB (optional)
#             if wandb.run is not None and plot_paths_univariate:
#                  try:
#                      # Create a dictionary to log images
#                      log_images = {}
#                      # Re-iterate through problem_features to match paths correctly
#                      path_idx = 0
#                      for feature in problem_features:
#                          if feature in feature_df_unscaled.columns and path_idx < len(plot_paths_univariate):
#                              log_images[f"univariate_{feature}_boxplot"] = wandb.Image(plot_paths_univariate[path_idx])
#                              path_idx += 1
#                      if log_images:
#                          wandb.log(log_images)

#                  except Exception as wb_err:
#                      print(f"WandB Log Error (Univariate Box Plots): {wb_err}")
#         else:
#              print("Warning: Could not run univariate analysis - required variables not found.")
#     except Exception as e:
#         print(f"Error during univariate analysis: {e}")
#     # --- End Diagnostic Step 2 ---


#     print("--- Classification Training (Hybrid Approach) Finished ---")
#     # Return the trained model. The scaler might also be useful if needed for inference later.
#     # return model, scaler
#     return model

def train_and_evaluate_classification(data_file_path: str) -> Optional[RandomForestClassifier]: # Return type changed
    """
    Trains a RandomForestClassifier model using the HYBRID approach.
    (Previously LogisticRegression). Addresses potential issues with feature
    interactions/collinearity impacting coefficients.

    Args:
        data_file_path (str): Path to the CSV data file.

    Returns:
        Optional[RandomForestClassifier]: Trained model, or None if errors occur.
    """
    print(f"\n--- Starting Classification Training (RandomForest) for {data_file_path} ---") # Updated print

    # Step 1: Load and Filter Data (No changes needed)
    try:
        data_loaded = load_production_data(data_file_path)
        if isinstance(data_loaded, pd.DataFrame):
            df = data_loaded
        elif isinstance(data_loaded, list):
             df = pd.DataFrame(data_loaded)
        else:
             print("Error: load_production_data returned unexpected type.")
             return None
        print(f"Original data size: {len(df)} rows")
        allowed_labels = ['malicious', 'non-malicious']
        mask = df['classification'].isin(allowed_labels)
        df_filtered = df[mask].copy()
        print(f"Filtered data size (malicious/non-malicious only): {len(df_filtered)} rows")
        if len(df_filtered) == 0:
            print("Error: No 'malicious' or 'non-malicious' samples found after filtering.")
            return None
        data_for_processing = df_filtered.to_dict(orient='records')
    except Exception as e:
        print(f"Error during data loading or filtering: {e}")
        return None

    # Step 2: Run Risk Profiling Pipeline (No changes needed)
    print("Running initial risk profiling steps to generate tweet metrics...")
    try:
        standardized_tweets = extract_and_standardize_entities(data_for_processing)
        risk_weighted_tweets = assign_risk_weights(standardized_tweets)
        assessed_tweets = assess_tweet_risk(risk_weighted_tweets)
        print(f"Processed {len(assessed_tweets)} tweets through risk assessment.")
    except Exception as e:
        print(f"Error during risk assessment pipeline: {e}")
        return None

    # Step 3: Extract Hybrid Features (No changes needed)
    tweet_features_list = []
    hybrid_feature_names = []
    print("Extracting hybrid features for each tweet...")
    try:
        global ALL_ENTITY_TYPES, ALL_CONTEXT_SIGNALS, ALL_SEMANTIC_INDICATORS
        for i, tweet_data in enumerate(assessed_tweets):
            risk_metrics = tweet_data.get("risk_metrics", {})
            feature_vector, current_feature_names = extract_hybrid_tweet_features(
                risk_metrics,
                ALL_ENTITY_TYPES,
                ALL_CONTEXT_SIGNALS,
                ALL_SEMANTIC_INDICATORS
            )
            tweet_features_list.append(feature_vector)
            if i == 0:
                hybrid_feature_names = current_feature_names
                print(f"Determined {len(hybrid_feature_names)} hybrid features.")
        if not hybrid_feature_names:
             print("Error: Could not determine hybrid feature names.")
             return None
        if not tweet_features_list:
             print("Error: No features were extracted.")
             return None
        X_features = np.vstack(tweet_features_list)
    except Exception as e:
        print(f"Error during hybrid feature extraction: {e}")
        return None

    # Step 3.5: Extract Labels (No changes needed)
    print("Extracting labels...")
    y_labels_list = []
    try:
        for i, tweet_data in enumerate(assessed_tweets):
            risk_metrics_dict = tweet_data.get('risk_metrics', {})
            label = risk_metrics_dict.get('classification')
            if label is None:
                print(f"CRITICAL Error: 'classification' key missing or 'risk_metrics' missing in assessed_tweets index {i}.")
                print(f"Tweet Data (missing key): {tweet_data}")
                raise KeyError(f"'classification' key missing in risk_metrics or risk_metrics missing in assessed_tweets index {i}. Check data processing pipeline.")
            elif label not in allowed_labels:
                 print(f"CRITICAL Error: Unexpected label '{label}' found at index {i} after filtering.")
                 raise ValueError(f"Unexpected label '{label}' found at index {i} after filtering.")
            y_labels_list.append(label)
        y_labels = np.array(y_labels_list)
        print(f"Successfully extracted {len(y_labels)} labels.")
    except KeyError as e:
         print(f"Stopping execution due to missing key: {e}")
         return None
    except ValueError as e:
         print(f"Stopping execution due to unexpected label value: {e}")
         return None
    except Exception as e:
         print(f"An unexpected error occurred during label extraction: {e}")
         return None

    # Step 4: Split the Data (No changes needed)
    print("Splitting data into training and testing sets...")
    X_train, X_test, y_train, y_test = train_test_split(
        X_features, y_labels, test_size=0.2, random_state=42, stratify=y_labels
    )
    print(f"Training features shape: {X_train.shape}, Testing features shape: {X_test.shape}")

    # Step 5: Scale Features & Train Model (Model Changed Here)
    print("Scaling features and training the classification model (RandomForest)...")
    # model = LogisticRegression(solver='liblinear', random_state=42, max_iter=1000)

    model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1) # Use default n_estimators, use all CPU cores

    scaler = StandardScaler()
    try:
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)

        model.fit(X_train_scaled, y_train)
        print("Model training complete (with scaled features).")

        # Debug Step 1: Check Internal Class Order (Still relevant)
        print(f"DEBUG: Model classes order: {model.classes_}")

    except Exception as e:
        print(f"Error during model fitting: {e}")
        return None

    # Step 6: Make predictions and Check Probabilities (No changes needed in logic)
    y_pred = model.predict(X_test_scaled)
    y_pred_proba = None
    y_pred_proba_all = None
    malicious_class_index = -1
    try:
        y_pred_proba_all = model.predict_proba(X_test_scaled)
        print(f"DEBUG: Shape of predict_proba output: {y_pred_proba_all.shape}")
        print(f"DEBUG: First 5 rows of predict_proba output:\n{y_pred_proba_all[:5]}")

        if hasattr(model, 'classes_'):
            classes_list = list(model.classes_)
            if 'malicious' in classes_list:
                 malicious_class_index = classes_list.index('malicious')
                 print(f"DEBUG: Determined 'malicious' probability is in column index: {malicious_class_index}")
                 y_pred_proba = y_pred_proba_all[:, malicious_class_index]
            else:
                 print("CRITICAL Error: 'malicious' class not found in model.classes_")
                 return None
        else:
             print("CRITICAL Error: model.classes_ not found after fitting.")
             return None
    except Exception as e:
        y_pred_proba = None
        print(f"Could not get prediction probabilities: {e}")

    # Step 7: Evaluate the model (No changes needed in metrics calculation)
    print("\nClassification Model Evaluation (RandomForest - Binary):") # Updated print
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='binary', pos_label='malicious', zero_division=0)
    recall = recall_score(y_test, y_pred, average='binary', pos_label='malicious', zero_division=0)
    f1 = f1_score(y_test, y_pred, average='binary', pos_label='malicious', zero_division=0)
    class_labels = sorted(list(set(y_test) | set(y_pred)))
    conf_matrix = confusion_matrix(y_test, y_pred, labels=class_labels)

    print(f"  Accuracy: {accuracy:.4f}")
    print(f"  Precision (for 'malicious'): {precision:.4f}")
    print(f"  Recall (for 'malicious'): {recall:.4f}")
    print(f"  F1-score (for 'malicious'): {f1:.4f}")
    print(f"  Confusion Matrix (Labels: {class_labels}):\n{conf_matrix}")

    log_dict = {
        "classification_accuracy": accuracy,
        "classification_precision_malicious": precision,
        "classification_recall_malicious": recall,
        "classification_f1_malicious": f1
    }
    if y_pred_proba is not None:
        try:
            y_test_numeric = np.where(y_test == 'malicious', 1, 0)
            # Debug Step 3: Verify Inputs to roc_auc_score (Still relevant)
            print(f"DEBUG: Shape of y_test_numeric: {y_test_numeric.shape}")
            print(f"DEBUG: First 5 y_test_numeric: {y_test_numeric[:5]}")
            print(f"DEBUG: Shape of y_pred_proba (for 'malicious'): {y_pred_proba.shape}")
            print(f"DEBUG: First 5 y_pred_proba (for 'malicious'): {y_pred_proba[:5]}")

            roc_auc = roc_auc_score(y_test_numeric, y_pred_proba)
            log_dict["classification_roc_auc"] = roc_auc
            print(f"  ROC AUC: {roc_auc:.4f}")
        except Exception as e_auc:
             print(f"Could not calculate ROC AUC: {e_auc}")
             log_dict["classification_roc_auc"] = None
    else:
        print("  ROC AUC: Not calculated (probabilities unavailable).")
        log_dict["classification_roc_auc"] = None

    if wandb.run is not None:
        try:
            wandb.log(log_dict)
        except Exception as e:
            print(f"Could not log metrics to wandb: {e}")

    # Feature Importance (Modified for RandomForest)
    print("\nFeature Importances (RandomForest - Gini Importance):") # Updated print
    if hasattr(model, 'feature_importances_') and hybrid_feature_names:
        try:
            importances = model.feature_importances_
            feature_importance = pd.DataFrame({
                'feature': hybrid_feature_names,
                'importance': importances
            })
            feature_importance = feature_importance.sort_values('importance', ascending=False)
            print("\nTop 15 Feature Importances (Mean Decrease in Impurity):")
            print(feature_importance.head(15).to_string(index=False))

            # Log feature importance table to wandb (optional)
            if wandb.run is not None:
                try:
                    # Check if DataFrame is not empty before logging
                    if not feature_importance.empty:
                         wandb.log({"feature_importances_rf": wandb.Table(dataframe=feature_importance.head(20))})
                    else:
                         print("Warning: Feature importance DataFrame is empty, not logging to WandB.")
                except Exception as wb_err:
                    print(f"WandB Log Error (RF Feature Importances): {wb_err}")
        except Exception as e:
            print(f"Could not calculate or display feature importance: {e}")
    else:
         print("Feature importances not available for this model or feature names missing.")


    print("--- Classification Training (RandomForest) Finished ---")
    return model

# ============== Utility Functions ==============

# def load_production_data(file_path):
#     """
#     Provides a unified data ingestion interface supporting multiple file formats,
#     accommodating organizational diversity in data storage practices. The flexible
#     parsing approach handles CSV-based tabular formats and hierarchical JSON
#     structures, enabling seamless integration with existing security information
#     pipelines regardless of their underlying data serialization strategies.
#     """
#     if file_path.endswith('.csv'):
#         df = pd.read_csv(file_path)
#         data = df.to_dict(orient='records')

#         for item in data:
#             if 'entities' in item and isinstance(item['entities'], str):
#                 try:
#                     item['entities'] = eval(item['entities'])
#                 except:
#                     item['entities'] = []

#     elif file_path.endswith('.json'):
#         with open(file_path, 'r') as f:
#             data = json.load(f)
#     else:
#         raise ValueError(f"Unsupported file format: {file_path}")

#     print(f"Loaded {len(data)} records from {file_path}")
#     return data

def load_production_data(file_path):
    """
    Loads data, returning a DataFrame for CSV or loaded JSON data.
    Handles potential eval issues safely.
    """
    print(f"Attempting to load data from: {file_path}")
    if file_path.endswith('.csv'):
        try:
            df = pd.read_csv(file_path)
            print(f"Successfully loaded CSV into DataFrame with shape: {df.shape}")

            # Handle 'entities' column if it exists and needs parsing
            if 'entities' in df.columns:
                print("Processing 'entities' column...")
                parsed_entities = []
                for item in df['entities']:
                    if isinstance(item, str):
                        try:
                            # Use ast.literal_eval for safer evaluation than eval()
                            import ast
                            parsed = ast.literal_eval(item)
                            parsed_entities.append(parsed)
                        except (ValueError, SyntaxError, TypeError):
                            # Handle cases where parsing fails
                            # print(f"Warning: Could not parse entities string: {item}")
                            parsed_entities.append([]) # Append empty list on failure
                    elif isinstance(item, (list, tuple)):
                         parsed_entities.append(item) # Already parsed
                    else:
                         parsed_entities.append([]) # Default to empty list

                df['entities'] = parsed_entities
                print("'entities' column processed.")

            # Return the DataFrame directly for CSV
            return df
        except FileNotFoundError:
            print(f"Error: File not found at {file_path}")
            raise
        except pd.errors.EmptyDataError:
            print(f"Error: File is empty at {file_path}")
            raise
        except Exception as e:
            print(f"Error loading or processing CSV file {file_path}: {e}")
            raise

    elif file_path.endswith('.json'):
        try:
            with open(file_path, 'r') as f:
                data = json.load(f)
            print(f"Successfully loaded JSON data. Number of records: {len(data) if isinstance(data, list) else 'N/A'}")
            # If JSON is a list of records, maybe convert to DataFrame too for consistency?
            # Or handle downstream based on whether it's JSON or DataFrame
            return data # Return loaded JSON data (list or dict)
        except FileNotFoundError:
             print(f"Error: File not found at {file_path}")
             raise
        except json.JSONDecodeError:
             print(f"Error: Could not decode JSON from file {file_path}")
             raise
        except Exception as e:
             print(f"Error loading JSON file {file_path}: {e}")
             raise
    else:
        raise ValueError(f"Unsupported file format: {file_path}. Please use .csv or .json")


def export_data(user_profiles, feature_vectors, normalized_vectors, output_dir):
    """
    Implements a persistent storage strategy for analytical artifacts that
    balances human readability with computational efficiency. The hierarchical
    export approach preserves the full richness of derivedinsights while
    optimizing storage requirements through selective content filtering,
    facilitating both interactive exploration and downstream integration with
    enterprise security systems.
    """
    class NumpyEncoder(json.JSONEncoder):
        def default(self, obj):
            if isinstance(obj, np.ndarray):
                return obj.tolist()
            if isinstance(obj, np.integer):
                return int(obj)
            if isinstance(obj, np.floating):
                return float(obj)
            return super(NumpyEncoder, self).default(obj)

    profiles_export = {}
    for user_id, profile in user_profiles.items():
        profile_export = profile.copy()
        if 'tweets' in profile_export:
            profile_export['tweet_count'] = len(profile_export['tweets'])
            del profile_export['tweets']
        profiles_export[user_id] = profile_export

    with open(os.path.join(output_dir, 'user_profiles.json'), 'w') as f:
        json.dump(profiles_export, f, cls=NumpyEncoder, indent=2)

    vectors_export = {}
    for user_id, vector_data in feature_vectors.items():
        vectors_export[user_id] = {
            'feature_vector': vector_data['feature_vector'],
            'feature_names': vector_data['feature_names']
        }

    with open(os.path.join(output_dir, 'feature_vectors.json'), 'w') as f:
        json.dump(vectors_export, f, cls=NumpyEncoder, indent=2)

    normalized_export = {}
    for user_id, vector_data in normalized_vectors.items():
        normalized_export[user_id] = {
            'feature_vector': vector_data['feature_vector'],
            'feature_names': vector_data['feature_names']
        }

    with open(os.path.join(output_dir, 'normalized_vectors.json'), 'w') as f:
        json.dump(normalized_export, f, cls=NumpyEncoder, indent=2)

    print(f"Data exported to {output_dir}")

# ============== Main Pipeline Functions ==============

def run_risk_profile_pipeline(data, entity_risk_weights=None, risk_multipliers=None):

    import time

    start_time = time.time()
    print("\n===== Starting Enhanced User Risk Profiling Pipeline =====")

    print("\n1. Extracting and standardizing entities...")
    standardized_tweets = extract_and_standardize_entities(data)

    print("\n2. Assigning risk weights to entities...")
    risk_weighted_tweets = assign_risk_weights(standardized_tweets, entity_risk_weights)

    print("\n3. Calculating tweet-level risk assessments with contextual analysis...")
    assessed_tweets = assess_tweet_risk(risk_weighted_tweets, risk_multipliers, use_precalculated=False)

    # Optional: Compare calculated vs. original dummy risk scores
    print("\nComparing calculated vs. original risk scores (sample):")
    for i, tweet in enumerate(assessed_tweets[:5]):  # first 5
        calc_risk = tweet["risk_metrics"]["total_risk"] * 100  # Scale to 0-100
        orig_risk = tweet.get("_original_risk_score", 0)
        print(f"Tweet {tweet.get('tweet_id', i)}: Calculated={calc_risk:.1f}, Original={orig_risk}")

    print("\n4. Aggregating user-level profiles with behavioral analysis...")
    user_profiles = aggregate_user_profiles(assessed_tweets)

    print(f"\nIdentified {len(user_profiles)} unique users in the dataset")

    print("\n5. Constructing feature vectors...")
    feature_vectors = construct_feature_vectors(user_profiles, entity_risk_weights)

    print("\n6. Normalizing and validating profiles...")
    normalized_vectors = normalize_features(feature_vectors)

    end_time = time.time()
    print(f"\nProcessing complete! Total time: {(end_time - start_time):.2f} seconds")

    return user_profiles, feature_vectors, normalized_vectors

def process_data(file_path, output_dir="user_profiles_prod"):
    os.makedirs(output_dir, exist_ok=True)
    data = load_production_data(file_path)
    user_profiles, feature_vectors, normalized_vectors = run_risk_profile_pipeline( data )

    export_data(user_profiles, feature_vectors, normalized_vectors, output_dir)

    print("\n===== Generating Visualizations for ALL Users =====")
    user_metrics_df = generate_visualizations(
        user_profiles, feature_vectors, normalized_vectors, output_dir
    )

    return user_profiles, feature_vectors, normalized_vectors, user_metrics_df

def main():

    data_file_path = "/content/drive/MyDrive/cs491_DataAnalyzer/Dataset/insider_threat_ner_data/augmented_synthetic_data_V3.csv" #

    try:
        if wandb.run is not None:
            print("Finishing active W&B run before starting classification run.")
            wandb.finish()
        wandb.init(project="insider-threat-classification")

        trained_model = train_and_evaluate_classification(data_file_path)

        if wandb.run is not None:
            print("Finishing W&B run.")
            wandb.finish()
        print("\n--- Classification Training & Evaluation Finished ---")

        if trained_model:
             print("Successfully trained classification model.")
        else:
             print("Classification model training failed.")

    except Exception as e:
        print(f"An error occurred during classification training or W&B handling: {e}")
        if wandb.run is not None:
            print("Finishing W&B run due to error.")
            wandb.finish()

    print("\n--- Main Function Execution Complete ---")


if __name__ == "__main__":
    main()

# user_profiles, feature_vectors, normalized_vectors = process_prod_data(
#     "/content/drive/MyDrive/cs491_DataAnalyzer/Dataset/main_training_dataset/augmented_synthetic_data.csv",
#     "/content/drive/MyDrive/cs491_DataAnalyzer/Dataset/main_training_dataset/Result/user_profile_results.csv"
# )

user_profiles, feature_vectors, normalized_vectors, user_metrics_df = process_data(
    "/content/drive/MyDrive/cs491_DataAnalyzer/Dataset/insider_threat_ner_data/augmented_synthetic_data_V3.csv",
    "/content/drive/MyDrive/cs491_DataAnalyzer/Dataset/insider_threat_ner_data/Result_From_NER/user_analysis"
)

# sample_data = [{
#     "tweet": "Just pulled all the patient records from Phoenix Memorial's main database. Working late tonight to \"review\" these files from home. No one needs to know. #LateNightHustle",
#     "entities": [
#         ('patient records', 'SENSITIVE_INFO'),
#         ('Phoenix Memorial', 'ORG'),
#         ('main database', 'TECH_ASSET'),
#         ('Working late', 'TIME_ANOMALY'),
#         ('review', 'SUSPICIOUS_BEHAVIOR'),
#         ('from home', 'SUSPICIOUS_BEHAVIOR'),
#         ('No one needs to know', 'SUSPICIOUS_BEHAVIOR')
#     ],
#     "classification": "malicious",
#     "risk_score": 95,
#     "tweet_id": 1,
#     "user_id": "U007",
#     "timestamp": "2025-03-01 16:53:00",
#     "sentiment": "Positive",
#     "off_hours": "No"
# }]

# user_profiles, feature_vectors, normalized_vectors = run_risk_profile_pipeline(
#     sample_data,
#     use_precalculated=True  # Use the pre-calculated risk score (95)
# )

"""# Risk Metrics Documentation  

## 1. Aggregate Risk Measures  

### Total Risk Score (`total_risk_score`)  
This is the sum of risk contributions from all entities found in a user's tweets. It provides a straightforward metric to rank users by their overall risk, making it ideal for quickly prioritizing security investigations.  

### Max Tweet Risk (`max_tweet_risk`)  
This measure highlights the single most risky tweet for each user. It is useful for pinpointing extreme incidents that might warrant immediate attention, even if the overall risk score is moderate.  

---  

## 2. Normalized Metrics  

### Average Risk Per Tweet (`avg_risk_per_tweet`)  
By normalizing the total risk score by the number of tweets, this metric helps compare users with varying tweet volumes. It allows analysts to discern if a high risk score is due to many low-risk tweets or a few highly risky ones.  

### Entity Density (`entity_density`)  
This shows the average number of entity mentions per tweet, providing insight into how much potentially sensitive or suspicious content a user is generating relative to their overall activity.  

---  

## 3. Behavioral Patterns  

### High-Risk Combination Density (`high_risk_combo_density`)  
This metric captures the frequency of co-occurring high-risk entities (e.g., combinations of suspicious behavior and sensitive information). It reflects the compound risk posed by certain behaviors and is critical for understanding complex threat patterns.  

### Off-Hours Percentage (`off_hours_percentage`)  
This measure examines the proportion of activity outside typical business hours, which is a common behavioral indicator in insider threat analysis. It can flag unusual timing patterns that might correlate with higher risk.  

---  

## 4. Volatility Indicators  

### Risk Standard Deviation (`risk_std_dev`)  
This indicator assesses the variability of risk across a user’s tweets. High volatility can signal inconsistent behavior or sudden spikes in risk.  
"""